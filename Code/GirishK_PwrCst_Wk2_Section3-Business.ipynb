{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b21dea8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T23:01:18.561172Z",
     "iopub.status.busy": "2025-08-18T23:01:18.560741Z",
     "iopub.status.idle": "2025-08-18T23:01:31.938043Z",
     "shell.execute_reply": "2025-08-18T23:01:31.937485Z",
     "shell.execute_reply.started": "2025-08-18T23:01:18.561150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Powercast] Using input: /home/6376f5a9-d12b-4255-9426-c0091ad440a7/Powercast/results/Wk02_Section2/features/engineered_lag_rolling_imputed.csv (source=Section2_imputed)\n",
      "{\n",
      "  \"std_train\": \"/home/6376f5a9-d12b-4255-9426-c0091ad440a7/Powercast/results/Wk02_Section3/features/scaled_standard_train.csv\",\n",
      "  \"std_test\": \"/home/6376f5a9-d12b-4255-9426-c0091ad440a7/Powercast/results/Wk02_Section3/features/scaled_standard_test.csv\",\n",
      "  \"mm_train\": \"/home/6376f5a9-d12b-4255-9426-c0091ad440a7/Powercast/results/Wk02_Section3/features/scaled_minmax_train.csv\",\n",
      "  \"mm_test\": \"/home/6376f5a9-d12b-4255-9426-c0091ad440a7/Powercast/results/Wk02_Section3/features/scaled_minmax_test.csv\",\n",
      "  \"rb_train\": \"/home/6376f5a9-d12b-4255-9426-c0091ad440a7/Powercast/results/Wk02_Section3/features/scaled_robust_train.csv\",\n",
      "  \"rb_test\": \"/home/6376f5a9-d12b-4255-9426-c0091ad440a7/Powercast/results/Wk02_Section3/features/scaled_robust_test.csv\",\n",
      "  \"scalers_json\": \"/home/6376f5a9-d12b-4255-9426-c0091ad440a7/Powercast/results/Wk02_Section3/features/scalers.json\",\n",
      "  \"section_report\": \"/home/6376f5a9-d12b-4255-9426-c0091ad440a7/Powercast/results/Wk02_Section3/reports/SDS-CP036-powercast_Wk02_Section3_Business_Report.md\",\n",
      "  \"week_report\": \"/home/6376f5a9-d12b-4255-9426-c0091ad440a7/Powercast/SDS-CP036-powercast_Wk02_Report_Business.md\",\n",
      "  \"readme\": \"/home/6376f5a9-d12b-4255-9426-c0091ad440a7/Powercast/README.md\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Week 2 — Section 3: Feature Scaling & Normalization (single-cell, business-friendly)\n",
    "from pathlib import Path\n",
    "import os, re, json\n",
    "import pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "SECTION = \"Wk02_Section3\"\n",
    "WEEK_REPORT_FILENAME = \"SDS-CP036-powercast_Wk02_Report_Business.md\"\n",
    "SECTION_REPORT_FILENAME = \"SDS-CP036-powercast_Wk02_Section3_Business_Report.md\"\n",
    "\n",
    "def find_base_dir(start: Path) -> Path:\n",
    "    env = os.getenv(\"POWERCAST_BASE_DIR\")\n",
    "    if env and (Path(env)/\"Code\").exists():\n",
    "        return Path(env).resolve()\n",
    "    p = start.resolve()\n",
    "    for _ in range(8):\n",
    "        if (p/\"Code\").exists(): return p\n",
    "        if p.name.lower()==\"powercast\": return p\n",
    "        p = p.parent\n",
    "    return start.resolve()\n",
    "\n",
    "def _setup_dirs(base_dir: Path):\n",
    "    out_dir = base_dir / \"results\" / SECTION\n",
    "    features_dir = out_dir / \"features\"\n",
    "    plots_dir = out_dir / \"plots\"\n",
    "    reports_dir = out_dir / \"reports\"\n",
    "    for d in (out_dir, features_dir, plots_dir, reports_dir):\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "    # clean prior artifacts in this section only\n",
    "    for folder in (features_dir, plots_dir, reports_dir):\n",
    "        for p in folder.glob(\"*\"):\n",
    "            try:\n",
    "                if p.is_file(): p.unlink()\n",
    "            except: pass\n",
    "    return out_dir, features_dir, plots_dir, reports_dir\n",
    "\n",
    "def _pick_input(base_dir: Path):\n",
    "    # Prefer Section 2 imputed, then raw, else original Tetuan CSV\n",
    "    s2_imp = base_dir/\"results\"/\"Wk02_Section2\"/\"features\"/\"engineered_lag_rolling_imputed.csv\"\n",
    "    s2_raw = base_dir/\"results\"/\"Wk02_Section2\"/\"features\"/\"engineered_lag_rolling.csv\"\n",
    "    if s2_imp.exists(): return s2_imp, \"Section2_imputed\"\n",
    "    if s2_raw.exists(): return s2_raw, \"Section2_raw\"\n",
    "    orig = base_dir/\"data\"/\"Tetuan City power consumption.csv\"\n",
    "    if orig.exists(): return orig, \"original\"\n",
    "    # fallback to any csv in data\n",
    "    any_csv = list((base_dir/\"data\").glob(\"*.csv\"))\n",
    "    if any_csv: return any_csv[0], \"original\"\n",
    "    raise FileNotFoundError(\"No suitable input found: expected Section2 features or data CSV under <BASE>/data.\")\n",
    "\n",
    "def _find_datetime_column(df: pd.DataFrame):\n",
    "    for c in [\"DateTime\",\"datetime\",\"date_time\",\"Timestamp\",\"timestamp\",\"time\",\"Date\",\"date\"]:\n",
    "        if c in df.columns: return c\n",
    "    for c in df.columns:\n",
    "        if any(k in c.lower() for k in [\"date\",\"time\",\"stamp\"]): return c\n",
    "    return None\n",
    "\n",
    "def _ensure_dt(df: pd.DataFrame, dt_col: str):\n",
    "    dt = pd.to_datetime(df[dt_col], errors=\"coerce\")\n",
    "    if dt.isna().any():\n",
    "        dt2 = pd.to_datetime(df[dt_col], errors=\"coerce\", dayfirst=True)\n",
    "        dt = dt.fillna(dt2)\n",
    "    if dt.isna().any(): raise ValueError(\"Unable to parse timestamps.\")\n",
    "    return dt\n",
    "\n",
    "def _train_test_split_time(df: pd.DataFrame, dt_col: str, test_size=0.2):\n",
    "    n = len(df); split = max(1, int(n*(1-test_size)))\n",
    "    return df.iloc[:split].copy(), df.iloc[split:].copy()\n",
    "\n",
    "def _choose_numeric_feature_columns(df: pd.DataFrame, dt_col: str):\n",
    "    num_cols = [c for c in df.columns if c!=dt_col and pd.api.types.is_numeric_dtype(df[c])]\n",
    "    # binary flags stay unscaled\n",
    "    bin_like = []\n",
    "    for c in list(num_cols):\n",
    "        vals = pd.Series(df[c]).dropna().unique()\n",
    "        if len(vals)<=3 and set(vals).issubset({0,1}):\n",
    "            bin_like.append(c)\n",
    "    # cyclic encodings in [-1,1] leave unscaled\n",
    "    cyc_like = [c for c in num_cols if any(x in c.lower() for x in [\"sin_\", \"cos_\"])]\n",
    "    skip_cols = sorted(set(bin_like+cyc_like))\n",
    "    scale_cols = [c for c in num_cols if c not in skip_cols]\n",
    "    return scale_cols, skip_cols\n",
    "\n",
    "def _scale_safe(train, test, cols, method):\n",
    "    if not cols: \n",
    "        return train.copy(), test.copy(), {\"columns\": [], \"method\": method}\n",
    "    if method==\"standard\": scaler = StandardScaler()\n",
    "    elif method==\"minmax\": scaler = MinMaxScaler()\n",
    "    elif method==\"robust\": scaler = RobustScaler()\n",
    "    else: raise ValueError(\"unknown method\")\n",
    "    scaler.fit(train[cols])\n",
    "    tr = train.copy(); te = test.copy()\n",
    "    tr[cols] = scaler.transform(train[cols])\n",
    "    te[cols] = scaler.transform(test[cols])\n",
    "    params = {\"columns\": cols, \"method\": method}\n",
    "    if hasattr(scaler,\"mean_\"): params[\"mean_\"]=scaler.mean_.tolist()\n",
    "    if hasattr(scaler,\"scale_\"): params[\"scale_\"]=scaler.scale_.tolist()\n",
    "    if hasattr(scaler,\"center_\"): \n",
    "        try: params[\"center_\"]=scaler.center_.tolist()\n",
    "        except: params[\"center_\"]=None\n",
    "    if hasattr(scaler,\"quantile_range\"): \n",
    "        try: params[\"quantile_range\"]=list(scaler.quantile_range)\n",
    "        except: pass\n",
    "    return tr, te, params\n",
    "\n",
    "def _plot_hist_before_after(train_before, train_after, cols, plots_dir, prefix):\n",
    "    if not cols: return None, None\n",
    "    c = cols[0]\n",
    "    plt.figure(); pd.Series(train_before[c]).dropna().hist(bins=40); \n",
    "    plt.title(f\"{prefix}: BEFORE scaling — {c}\"); plt.xlabel(c); plt.ylabel(\"count\")\n",
    "    p1 = plots_dir/f\"{prefix.lower()}_{c}_before.png\"; plt.savefig(p1, bbox_inches=\"tight\"); plt.close()\n",
    "    plt.figure(); pd.Series(train_after[c]).dropna().hist(bins=40);\n",
    "    plt.title(f\"{prefix}: AFTER scaling — {c}\"); plt.xlabel(c); plt.ylabel(\"count\")\n",
    "    p2 = plots_dir/f\"{prefix.lower()}_{c}_after.png\"; plt.savefig(p2, bbox_inches=\"tight\"); plt.close()\n",
    "    return p1, p2\n",
    "\n",
    "def _to_serializable(obj):\n",
    "    import numpy as np\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    if isinstance(obj, (list, tuple)):\n",
    "        return [_to_serializable(x) for x in obj]\n",
    "    if isinstance(obj, dict):\n",
    "        return {k:_to_serializable(v) for k,v in obj.items()}\n",
    "    return obj\n",
    "\n",
    "def _write_section_report(reports_dir: Path, csv_name: str, diagnostics: dict, scaled_cols, skipped_cols, plots):\n",
    "    # Business-friendly Q&A copy\n",
    "    q1 = (\"We prepared the numbers so different features are on comparable scales, which helps models learn reliably and keeps one large-magnitude feature from dominating the rest. \"\n",
    "          \"We used three common approaches (each with a clear use case):\\n\"\n",
    "          \"- **Standardization (z-score)** – centers at 0 and scales by typical spread (standard deviation). Good default when data is roughly bell-shaped.\\n\"\n",
    "          \"- **Min–Max scaling** – compresses values to a 0–1 range. Helpful for algorithms sensitive to absolute ranges.\\n\"\n",
    "          \"- **Robust scaling** – centers by the median and scales by the IQR (inter-quartile range); more resistant to outliers and skew.\\n\"\n",
    "          \"Using all three gives the team flexibility to pick the best fit for downstream modeling without re-running feature engineering.\")\n",
    "    q2 = (\"We guarded against **data leakage**—letting future information seep into training—by splitting the data **by time**, \"\n",
    "          \"fitting each scaler **only on the training period**, and applying that scaling to the later **test period**. \"\n",
    "          \"This mirrors production and keeps evaluation honest.\")\n",
    "    if skipped_cols:\n",
    "        special = (\"We intentionally **left some features unscaled** because they are binary flags (0/1) or already unitless cyclical encodings (sine/cosine in [-1,1]). \"\n",
    "                   f\"Examples: {', '.join(skipped_cols[:8])}\" + (\" ...\" if len(skipped_cols)>8 else \"\") +\n",
    "                   \". For outlier‑prone metrics we prefer **Robust scaling** to avoid over‑weighting spikes.\")\n",
    "    else:\n",
    "        special = (\"We intentionally **left binary flags** and **cyclical encodings** unscaled; for outlier‑prone metrics we prefer **Robust scaling**.\")\n",
    "    lines = [\n",
    "        \"# Week 2 — Section 3: Feature Scaling & Normalization\",\n",
    "        \"\",\n",
    "        f\"**Input dataset:** `{csv_name}`\",\n",
    "        f\"**Rows:** {diagnostics.get('rows')} | **Period:** {diagnostics.get('start')} → {diagnostics.get('end')}\",\n",
    "        \"\",\n",
    "        \"## Key Questions Answered\",\n",
    "        \"### 3. Feature Scaling & Normalization\",\n",
    "        \"Q: Which normalization or scaling techniques did you apply to your numerical features, and why?\",\n",
    "        \"A: \" + q1,\n",
    "        \"\",\n",
    "        \"Q: How did you ensure that scaling was performed without introducing data leakage?\",\n",
    "        \"A: \" + q2,\n",
    "        \"\",\n",
    "        \"Q: Did you notice any features that required special treatment during normalization?\",\n",
    "        \"A: \" + special,\n",
    "        \"\",\n",
    "        \"## Artifacts\",\n",
    "        \"- Scaled features (Standard): `features/scaled_standard_train.csv`, `features/scaled_standard_test.csv`\",\n",
    "        \"- Scaled features (MinMax): `features/scaled_minmax_train.csv`, `features/scaled_minmax_test.csv`\",\n",
    "        \"- Scaled features (Robust): `features/scaled_robust_train.csv`, `features/scaled_robust_test.csv`\",\n",
    "        \"- Scaler parameters: `features/scalers.json`\",\n",
    "        f\"- Plots: {[Path(p).name for p in plots if p]}\",\n",
    "        \"- Machine-readable summary: `summary.json`\"\n",
    "    ]\n",
    "    rp = reports_dir/SECTION_REPORT_FILENAME\n",
    "    rp.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "    return rp\n",
    "\n",
    "def _update_week_report(base_dir: Path, section_block_md: str):\n",
    "    wk_path = base_dir / WEEK_REPORT_FILENAME\n",
    "    if not wk_path.exists():\n",
    "        base = [\n",
    "            \"# SDS-CP036-powercast — Wk02 Consolidated Business Report\",\n",
    "            \"\",\n",
    "            f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "            \"\",\n",
    "            \"## Section 1 — (placeholder)\",\n",
    "            \"## Section 2 — (placeholder)\",\n",
    "            section_block_md,\n",
    "            \"## Section 4 — (placeholder)\",\n",
    "            \"## Section 5 — (placeholder)\",\n",
    "        ]\n",
    "        wk_path.write_text(\"\\n\".join(base), encoding=\"utf-8\")\n",
    "        return str(wk_path)\n",
    "    txt = wk_path.read_text(encoding=\"utf-8\")\n",
    "    sec_pat = re.compile(r\"(## Section 3[\\s\\S]*?)(?=^## |\\Z)\", re.MULTILINE)\n",
    "    block = section_block_md + \"\\n\"\n",
    "    if \"## Section 3\" in txt and sec_pat.search(txt):\n",
    "        txt = sec_pat.sub(block, txt)\n",
    "    else:\n",
    "        txt += (\"\\n\" if not txt.endswith(\"\\n\") else \"\") + block\n",
    "    wk_path.write_text(txt, encoding=\"utf-8\")\n",
    "    return str(wk_path)\n",
    "\n",
    "def _update_readme(base_dir: Path, section_report_path: Path, plots):\n",
    "    readme = base_dir/\"README.md\"\n",
    "    md = readme.read_text(encoding=\"utf-8\") if readme.exists() else \"# Powercast — Project Overview\\n\\n## Table of Contents\\n\"\n",
    "    def _find(md, hdr):\n",
    "        m = re.search(rf\"(^## {re.escape(hdr)}\\\\s*$)\", md, flags=re.MULTILINE)\n",
    "        if not m: return None, None\n",
    "        start = m.end(); n = re.search(r\"^## \", md[start:], flags=re.MULTILINE)\n",
    "        end = start + (n.start() if n else len(md[start:]))\n",
    "        return start, end\n",
    "    def _insert(md, hdr, block):\n",
    "        s,e = _find(md, hdr)\n",
    "        if s is None: return md.rstrip()+f\"\\n\\n## {hdr}\\n\\n{block}\\n\"\n",
    "        body = md[s:e]\n",
    "        if block.strip() in body: return md\n",
    "        return md[:e] + (\"\\n\" if not body.endswith(\"\\n\") else \"\") + block + \"\\n\" + md[e:]\n",
    "    thumbs = []\n",
    "    for p in plots:\n",
    "        if p:\n",
    "            rel = Path(p).relative_to(base_dir).as_posix()\n",
    "            thumbs.append(f'<a href=\"./{rel}\"><img src=\"./{rel}\" width=\"260\" alt=\"Wk02_Section3 — {Path(p).name}\"></a>')\n",
    "    thumbs_block = \"\\n\".join(thumbs)\n",
    "    plots_block = \"### Wk02_Section3\\n\" + \"\\n\".join([f\"- [{Path(p).stem}](./{Path(p).relative_to(base_dir).as_posix()})\" for p in plots if p])\n",
    "    rel_rep = section_report_path.relative_to(base_dir).as_posix()\n",
    "    reps_block = f\"### Wk02_Section3\\n- [Week 2 – Section 3: Feature Scaling & Normalization](./{rel_rep})\"\n",
    "    md = _insert(md, \"Quick Gallery (click any thumbnail)\", thumbs_block)\n",
    "    md = _insert(md, \"Plots (grouped by Section)\", plots_block)\n",
    "    md = _insert(md, \"Section Reports (grouped)\", reps_block)\n",
    "    readme.write_text(md, encoding=\"utf-8\")\n",
    "    return str(readme)\n",
    "\n",
    "def process(base_dir: Path):\n",
    "    base_dir = Path(base_dir)\n",
    "    out_dir, features_dir, plots_dir, reports_dir = _setup_dirs(base_dir)\n",
    "\n",
    "    csv_path, src = _pick_input(base_dir)\n",
    "    df = pd.read_csv(csv_path)\n",
    "    dt_col = _find_datetime_column(df)\n",
    "    if dt_col is None: raise ValueError(\"No datetime-like timestamp column found.\")\n",
    "    df[dt_col] = _ensure_dt(df, dt_col)\n",
    "\n",
    "    # split\n",
    "    train, test = _train_test_split_time(df, dt_col, test_size=0.2)\n",
    "\n",
    "    # choose columns\n",
    "    scale_cols, skip_cols = _choose_numeric_feature_columns(train, dt_col)\n",
    "\n",
    "    # scale\n",
    "    tr_std, te_std, p_std = _scale_safe(train, test, scale_cols, \"standard\")\n",
    "    tr_mm,  te_mm,  p_mm  = _scale_safe(train, test, scale_cols, \"minmax\")\n",
    "    tr_rb,  te_rb,  p_rb  = _scale_safe(train, test, scale_cols, \"robust\")\n",
    "\n",
    "    # save datasets\n",
    "    std_train_csv = features_dir/\"scaled_standard_train.csv\"; tr_std.to_csv(std_train_csv, index=False)\n",
    "    std_test_csv  = features_dir/\"scaled_standard_test.csv\";  te_std.to_csv(std_test_csv, index=False)\n",
    "    mm_train_csv  = features_dir/\"scaled_minmax_train.csv\";   tr_mm.to_csv(mm_train_csv, index=False)\n",
    "    mm_test_csv   = features_dir/\"scaled_minmax_test.csv\";    te_mm.to_csv(mm_test_csv, index=False)\n",
    "    rb_train_csv  = features_dir/\"scaled_robust_train.csv\";   tr_rb.to_csv(rb_train_csv, index=False)\n",
    "    rb_test_csv   = features_dir/\"scaled_robust_test.csv\";    te_rb.to_csv(rb_test_csv, index=False)\n",
    "\n",
    "    # scalers params json\n",
    "    scalers_json = features_dir/\"scalers.json\"\n",
    "    with open(scalers_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\n",
    "            \"standard\": _to_serializable(p_std),\n",
    "            \"minmax\": _to_serializable(p_mm),\n",
    "            \"robust\": _to_serializable(p_rb),\n",
    "            \"skipped_columns\": skip_cols,\n",
    "            \"scaled_columns\": scale_cols,\n",
    "            \"input_source\": src\n",
    "        }, f, indent=2)\n",
    "\n",
    "    # plots\n",
    "    p_before, p_after = _plot_hist_before_after(train, tr_std, scale_cols, plots_dir, \"Standard\")\n",
    "\n",
    "    # diagnostics\n",
    "    dt = df[dt_col]\n",
    "    diagnostics = {\"rows\": int(len(df)), \"start\": str(dt.min()), \"end\": str(dt.max())}\n",
    "\n",
    "    # section report\n",
    "    section_report = _write_section_report(reports_dir, csv_path.name, diagnostics, scale_cols, skip_cols, [p for p in [p_before,p_after] if p])\n",
    "\n",
    "    # week report block\n",
    "    block = []\n",
    "    block.append(\"## Section 3 — Feature Scaling & Normalization\")\n",
    "    block.append(\"\")\n",
    "    block.append(\"### Key Questions Answered\")\n",
    "    block.append(\"Q: Which normalization or scaling techniques did you apply to your numerical features, and why?\")\n",
    "    block.append(\"A: We prepared the numbers so different features are on comparable scales, which helps models learn reliably and keeps one large-magnitude feature from dominating the rest. We used three common approaches (each with a clear use case): Standardization (z-score); Min–Max scaling; and Robust scaling. Using all three gives the team flexibility to pick the best fit for downstream modeling without re-running feature engineering.\")\n",
    "    block.append(\"\")\n",
    "    block.append(\"Q: How did you ensure that scaling was performed without introducing data leakage?\")\n",
    "    block.append(\"A: We guarded against data leakage by splitting the data by time, fitting each scaler only on the training period, and applying that scaling to the later test period. This mirrors production and keeps evaluation honest.\")\n",
    "    block.append(\"\")\n",
    "    block.append(\"Q: Did you notice any features that required special treatment during normalization?\")\n",
    "    if skip_cols:\n",
    "        block.append(\"A: Yes. We intentionally left some features unscaled because they are binary flags (0/1) or already unitless cyclical encodings (sine/cosine in [-1,1]). Examples: \" + \", \".join(skip_cols[:8]) + (\" ...\" if len(skip_cols)>8 else \"\") + \". For outlier‑prone metrics we prefer Robust scaling to avoid over‑weighting spikes.\")\n",
    "    else:\n",
    "        block.append(\"A: Yes. We intentionally left binary flags and cyclical encodings unscaled; for outlier‑prone metrics we prefer Robust scaling.\")\n",
    "    for p in [p_before,p_after]:\n",
    "        if p:\n",
    "            rel = Path(p).relative_to(base_dir).as_posix()\n",
    "            block.append(f\"![{Path(p).name}]({rel})\")\n",
    "    week_report = _update_week_report(base_dir, \"\\n\".join(block))\n",
    "\n",
    "    # README\n",
    "    readme = _update_readme(base_dir, section_report, [p for p in [p_before,p_after] if p])\n",
    "\n",
    "    print(f\"[Powercast] Using input: {csv_path} (source={src})\")\n",
    "    return {\n",
    "        \"std_train\": str(std_train_csv),\n",
    "        \"std_test\": str(std_test_csv),\n",
    "        \"mm_train\": str(mm_train_csv),\n",
    "        \"mm_test\": str(mm_test_csv),\n",
    "        \"rb_train\": str(rb_train_csv),\n",
    "        \"rb_test\": str(rb_test_csv),\n",
    "        \"scalers_json\": str(scalers_json),\n",
    "        \"section_report\": str(section_report),\n",
    "        \"week_report\": week_report,\n",
    "        \"readme\": readme\n",
    "    }\n",
    "\n",
    "# Execute\n",
    "BASE = find_base_dir(Path.cwd())\n",
    "info = process(BASE)\n",
    "print(json.dumps(info, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15cfd3d-8648-40a4-9c3f-8a4bf748273b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
