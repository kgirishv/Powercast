{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fd909ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T23:01:41.570753Z",
     "iopub.status.busy": "2025-08-18T23:01:41.570431Z",
     "iopub.status.idle": "2025-08-18T23:01:46.565923Z",
     "shell.execute_reply": "2025-08-18T23:01:46.565340Z",
     "shell.execute_reply.started": "2025-08-18T23:01:41.570732Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"train_csv\": \"/home/6376f5a9-d12b-4255-9426-c0091ad440a7/Powercast/results/Wk02_Section4/features/train.csv\",\n",
      "  \"test_csv\": \"/home/6376f5a9-d12b-4255-9426-c0091ad440a7/Powercast/results/Wk02_Section4/features/test.csv\",\n",
      "  \"walk_forward_csv\": \"/home/6376f5a9-d12b-4255-9426-c0091ad440a7/Powercast/results/Wk02_Section4/features/walk_forward_results.csv\",\n",
      "  \"plot\": \"/home/6376f5a9-d12b-4255-9426-c0091ad440a7/Powercast/results/Wk02_Section4/plots/wk02_section4_split_marker.png\",\n",
      "  \"section_report\": \"/home/6376f5a9-d12b-4255-9426-c0091ad440a7/Powercast/results/Wk02_Section4/reports/SDS-CP036-powercast_Wk02_Section4_Business_Report.md\",\n",
      "  \"week_report\": \"/home/6376f5a9-d12b-4255-9426-c0091ad440a7/Powercast/SDS-CP036-powercast_Wk02_Report_Business.md\",\n",
      "  \"readme\": \"/home/6376f5a9-d12b-4255-9426-c0091ad440a7/Powercast/README.md\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Week 2 — Section 4: Data Splitting & Preparation (Single‑cell, business‑friendly)\n",
    "from pathlib import Path\n",
    "import os, re, json\n",
    "import pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "SECTION = \"Wk02_Section4\"\n",
    "WEEK_REPORT_FILENAME = \"SDS-CP036-powercast_Wk02_Report_Business.md\"\n",
    "SECTION_REPORT_FILENAME = \"SDS-CP036-powercast_Wk02_Section4_Business_Report.md\"\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "def find_base_dir(start: Path) -> Path:\n",
    "    env = os.getenv(\"POWERCAST_BASE_DIR\")\n",
    "    if env and (Path(env)/\"Code\").exists():\n",
    "        return Path(env).resolve()\n",
    "    p = start.resolve()\n",
    "    for _ in range(8):\n",
    "        if (p/\"Code\").exists() and ((p/\"data\").exists() or (p/\"results\").exists()):\n",
    "            return p\n",
    "        if p.name.lower()==\"powercast\" and (p/\"Code\").exists():\n",
    "            return p\n",
    "        p = p.parent\n",
    "    return start.resolve()\n",
    "\n",
    "def _setup_dirs(base_dir: Path):\n",
    "    out_dir = base_dir / \"results\" / SECTION\n",
    "    features_dir = out_dir / \"features\"\n",
    "    plots_dir = out_dir / \"plots\"\n",
    "    reports_dir = out_dir / \"reports\"\n",
    "    for d in (out_dir, features_dir, plots_dir, reports_dir):\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "    return out_dir, features_dir, plots_dir, reports_dir\n",
    "\n",
    "def _clean_prev(*dirs: Path):\n",
    "    for folder in dirs:\n",
    "        if folder.exists():\n",
    "            for p in folder.glob(\"*\"):\n",
    "                try:\n",
    "                    if p.is_file(): p.unlink()\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "def _resolve_best_input(base_dir: Path):\n",
    "    # Prefer Section 3 split → Section 2 engineered (imputed → raw) → raw CSV\n",
    "    s3_train = base_dir/\"results\"/\"Wk02_Section3\"/\"features\"/\"scaled_standard_train.csv\"\n",
    "    s3_test  = base_dir/\"results\"/\"Wk02_Section3\"/\"features\"/\"scaled_standard_test.csv\"\n",
    "    if s3_train.exists() and s3_test.exists():\n",
    "        return \"section3\", (s3_train, s3_test)\n",
    "\n",
    "    s2_imp = base_dir/\"results\"/\"Wk02_Section2\"/\"features\"/\"engineered_lag_rolling_imputed.csv\"\n",
    "    s2_raw = base_dir/\"results\"/\"Wk02_Section2\"/\"features\"/\"engineered_lag_rolling.csv\"\n",
    "    if s2_imp.exists(): return \"section2_imputed\", (s2_imp,)\n",
    "    if s2_raw.exists(): return \"section2_raw\", (s2_raw,)\n",
    "\n",
    "    raw = base_dir/\"data\"/\"Tetuan City power consumption.csv\"\n",
    "    if raw.exists(): return \"raw\", (raw,)\n",
    "    raise FileNotFoundError(\"No suitable input found. Expected Section 3 or Section 2 outputs, or data/Tetuan City power consumption.csv.\")\n",
    "\n",
    "def _find_datetime_column(df: pd.DataFrame):\n",
    "    for c in [\"DateTime\",\"datetime\",\"date_time\",\"Timestamp\",\"timestamp\",\"time\",\"Date\",\"date\"]:\n",
    "        if c in df.columns: return c\n",
    "    for c in df.columns:\n",
    "        if any(k in c.lower() for k in [\"date\",\"time\",\"stamp\"]): return c\n",
    "    return None\n",
    "\n",
    "def _ensure_dt(series):\n",
    "    dt = pd.to_datetime(series, errors=\"coerce\")\n",
    "    if dt.isna().any():\n",
    "        dt2 = pd.to_datetime(series, errors=\"coerce\", dayfirst=True)\n",
    "        dt = dt.fillna(dt2)\n",
    "    return dt\n",
    "\n",
    "def _time_split(df: pd.DataFrame, dt_col: str, test_size=0.2):\n",
    "    n = len(df); split = int(n*(1-test_size))\n",
    "    train = df.iloc[:split].copy()\n",
    "    test  = df.iloc[split:].copy()\n",
    "    return train, test\n",
    "\n",
    "def _plot_time_coverage(df, dt_col, train, test, plots_dir: Path):\n",
    "    dt_all = _ensure_dt(df[dt_col]).ffill().bfill()\n",
    "    dt_tr  = _ensure_dt(train[dt_col]).ffill().bfill()\n",
    "    dt_te  = _ensure_dt(test[dt_col]).ffill().bfill()\n",
    "    if dt_tr.isna().all() or dt_te.isna().all():\n",
    "        return None  # skip plotting if datetimes are unusable\n",
    "    plt.figure()\n",
    "    plt.plot(dt_all, np.arange(len(dt_all)), label=\"timeline index\")\n",
    "    split_ts = pd.to_datetime(dt_tr.iloc[-1])\n",
    "    plt.axvline(split_ts, linestyle=\"--\", label=\"train/test split\")\n",
    "    plt.title(\"Chronological index & split marker\"); plt.xlabel(\"time\"); plt.ylabel(\"index\")\n",
    "    plt.legend()\n",
    "    p = plots_dir/\"wk02_section4_split_marker.png\"\n",
    "    plt.savefig(p, bbox_inches=\"tight\"); plt.close()\n",
    "    return p\n",
    "\n",
    "def _mae(y_true, y_pred):\n",
    "    m = (~pd.isna(y_true)) & (~pd.isna(y_pred))\n",
    "    if m.sum()==0: return float(\"nan\")\n",
    "    return float(np.mean(np.abs(y_true[m]-y_pred[m])))\n",
    "\n",
    "def _choose_target(df: pd.DataFrame, dt_col: str):\n",
    "    for cand in [\"Total\",\"total\",\"Total_kW\",\"Global_active_power\",\"Appliances\",\"Total_auto\"]:\n",
    "        if cand in df.columns: return cand\n",
    "    zones = [c for c in df.columns if (\"zone\" in c.lower() and pd.api.types.is_numeric_dtype(df[c]))]\n",
    "    if zones:\n",
    "        df[\"Total_auto\"] = df[zones].sum(axis=1, numeric_only=True)\n",
    "        return \"Total_auto\"\n",
    "    num_cols = [c for c in df.columns if c!=dt_col and pd.api.types.is_numeric_dtype(df[c])]\n",
    "    return num_cols[0] if num_cols else None\n",
    "\n",
    "def _walk_forward_folds(df: pd.DataFrame, dt_col: str, n_folds=3):\n",
    "    n = len(df)\n",
    "    if n < 50: return []\n",
    "    train_frac, test_frac = 0.6, 0.2\n",
    "    win = int(n*(train_frac+test_frac))\n",
    "    step = max(int(n*0.2), 1)\n",
    "    folds = []\n",
    "    start = 0\n",
    "    while len(folds) < n_folds and start+win <= n:\n",
    "        split = start+int(win*train_frac/(train_frac+test_frac))\n",
    "        tr = df.iloc[start:split].copy()\n",
    "        te = df.iloc[split:start+win].copy()\n",
    "        if len(te) > 5 and len(tr) > 5:\n",
    "            folds.append((tr, te))\n",
    "        start += step\n",
    "    return folds\n",
    "\n",
    "# ---------------- Business-friendly Q&A ----------------\n",
    "def _business_qna_text():\n",
    "    q1 = (\"We respected the **natural flow of time** in the data. \"\n",
    "          \"If training/testing files already existed from Section 3, we used them as-is. \"\n",
    "          \"Otherwise, we split by time: the **first 80% (earlier dates)** for training and the **last 20% (later dates)** for testing, \"\n",
    "          \"with **no shuffling**. This matches real-world usage, where yesterday teaches us to predict tomorrow.\")\n",
    "    q2 = (\"We prevented **information leakage**—that’s when future knowledge accidentally sneaks into training—by ensuring any learned settings \"\n",
    "          \"(such as scaling/normalization from earlier steps) were **fit on the training period only** and then **applied to the later test period**. \"\n",
    "          \"When building targets and features, we also avoided using any future-looking information. This keeps evaluation realistic for live operations.\")\n",
    "    q3 = (\"We validated the split in two ways: (1) a **timeline visualization** with a clear marker where training ends and testing begins; \"\n",
    "          \"and (2) **walk‑forward validation**, which repeatedly tests on successive future slices. \"\n",
    "          \"Think of it as asking, *“If we stopped here, could we predict the next step?”* \"\n",
    "          \"Consistent results across slices and a clean split line indicate the split is sound for forecasting.\")\n",
    "    return q1, q2, q3\n",
    "\n",
    "def _write_section_report(reports_dir: Path, csv_name: str, diagnostics: dict, qna: tuple[str,str,str], artifacts: dict, plot_path):\n",
    "    q1, q2, q3 = qna\n",
    "    lines = [\n",
    "        \"# Week 2 — Section 4: Data Splitting & Preparation\",\n",
    "        \"\",\n",
    "        f\"**Primary input:** `{csv_name}`\",\n",
    "        \"**\" + \" | \".join([\n",
    "            f\"Rows: {diagnostics.get('rows')}\",\n",
    "            f\"Train rows: {diagnostics.get('rows_train')}\",\n",
    "            f\"Test rows: {diagnostics.get('rows_test')}\",\n",
    "            f\"Target: {diagnostics.get('target')}\"\n",
    "        ]) + \"**\",\n",
    "        \"\",\n",
    "        \"## Key Questions Answered\",\n",
    "        \"### 4. Data Splitting & Preparation\",\n",
    "        \"Q: How did you split your data into training and test sets to maintain chronological order?\",\n",
    "        \"A: \" + q1,\n",
    "        \"\",\n",
    "        \"Q: What steps did you take to prevent information leakage between splits?\",\n",
    "        \"A: \" + q2,\n",
    "        \"\",\n",
    "        \"Q: How did you verify that your train/test split was appropriate for time-series forecasting?\",\n",
    "        \"A: \" + q3,\n",
    "        \"\",\n",
    "        \"## Artifacts\",\n",
    "        f\"- Train: `features/{Path(artifacts['train']).name}`\",\n",
    "        f\"- Test: `features/{Path(artifacts['test']).name}`\",\n",
    "        f\"- Walk-forward results: `features/{Path(artifacts['wf']).name}`\",\n",
    "        f\"- Plot: `{Path(plot_path).name if plot_path else 'n/a'}`\",\n",
    "        \"- Machine-readable summary: `summary.json`\"\n",
    "    ]\n",
    "    rp = reports_dir/SECTION_REPORT_FILENAME\n",
    "    rp.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "    return rp\n",
    "\n",
    "def _update_week_report(base_dir: Path, section_block_md: str):\n",
    "    wk_path = base_dir / WEEK_REPORT_FILENAME\n",
    "    if not wk_path.exists():\n",
    "        base = [\n",
    "            \"# SDS-CP036-powercast — Wk02 Consolidated Business Report (Inline Plots v2)\",\n",
    "            \"\",\n",
    "            f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "            f\"Project root: `{base_dir}`\",\n",
    "            \"\",\n",
    "            \"Includes Sections: 1, 2, 3, 4, 5\",\n",
    "            \"\",\n",
    "            \"## Section 1 — (placeholder)\",\n",
    "            \"\",\n",
    "            \"## Section 2 — (placeholder)\",\n",
    "            \"\",\n",
    "            \"## Section 3 — (placeholder)\",\n",
    "            \"\",\n",
    "            \"## Section 4 — Data Splitting & Preparation\",\n",
    "            \"\",\n",
    "            section_block_md,\n",
    "            \"\"\n",
    "        ]\n",
    "        wk_path.write_text(\"\\n\".join(base), encoding=\"utf-8\")\n",
    "        return str(wk_path)\n",
    "    txt = wk_path.read_text(encoding=\"utf-8\")\n",
    "    if \"## Section 4 — Data Splitting & Preparation\" in txt:\n",
    "        sec_pat = re.compile(r\"(## Section 4 — Data Splitting & Preparation[\\s\\S]*?)(?=^## |\\Z)\", re.MULTILINE)\n",
    "        if sec_pat.search(txt): txt = sec_pat.sub(section_block_md + \"\\n\", txt)\n",
    "        else: txt += \"\\n\" + section_block_md + \"\\n\"\n",
    "    else:\n",
    "        txt += \"\\n\" + section_block_md + \"\\n\"\n",
    "    wk_path.write_text(txt, encoding=\"utf-8\")\n",
    "    return str(wk_path)\n",
    "\n",
    "def _find_section_bounds(md: str, header_text: str):\n",
    "    pattern = re.compile(rf\"(^## {re.escape(header_text)}\\s*$)\", re.MULTILINE)\n",
    "    m = pattern.search(md)\n",
    "    if not m: return None, None\n",
    "    start = m.end()\n",
    "    n = re.compile(r\"^## \", re.MULTILINE).search(md, start)\n",
    "    end = n.start() if n else len(md)\n",
    "    return start, end\n",
    "\n",
    "def _insert_at_end_of_section(md: str, header_text: str, block: str) -> str:\n",
    "    if not block.strip(): return md\n",
    "    start, end = _find_section_bounds(md, header_text)\n",
    "    if start is None: return md.rstrip() + f\"\\n\\n## {header_text}\\n\\n{block.rstrip()}\\n\"\n",
    "    if block.strip() in md[start:end]: return md\n",
    "    return md[:end] + (\"\\n\" if not md[start:end].endswith(\"\\n\") else \"\") + block.rstrip() + \"\\n\" + md[end:]\n",
    "\n",
    "def _ensure_toc_item(md: str, title: str) -> str:\n",
    "    start, end = _find_section_bounds(md, \"Table of Contents\")\n",
    "    if start is None:\n",
    "        md = md.rstrip() + \"\\n\\n## Table of Contents\\n\\n\"\n",
    "        start, end = _find_section_bounds(md, \"Table of Contents\")\n",
    "    anchor = title.strip().lower().replace(\" \", \"-\")\n",
    "    bullet = f\"- [{title}](#{anchor})\"\n",
    "    body = md[start:end]\n",
    "    if bullet in body: return md\n",
    "    new = body.rstrip() + (\"\\n\" if body and not body.endswith(\"\\n\") else \"\") + bullet + \"\\n\"\n",
    "    return md[:start] + new + md[end:]\n",
    "\n",
    "def _update_readme(base_dir: Path, section_report_path: Path, plot_path):\n",
    "    readme = base_dir/\"README.md\"\n",
    "    md = readme.read_text(encoding=\"utf-8\") if readme.exists() else \"# Powercast — Project Overview\\n\\n## Table of Contents\\n\"\n",
    "\n",
    "    thumbs = []\n",
    "    if plot_path:\n",
    "        rel = Path(plot_path).relative_to(base_dir).as_posix()\n",
    "        thumbs.append(f'<a href=\"./{rel}\"><img src=\"./{rel}\" width=\"260\" alt=\"Wk02_Section4 — split marker\"></a>')\n",
    "    thumbs_block = \"\\n\".join(thumbs)\n",
    "\n",
    "    plots_block = \"### Wk02_Section4\\n\" + (\"\\n- [\" + Path(plot_path).stem + f\"](./{Path(plot_path).relative_to(base_dir).as_posix()})\" if plot_path else \"\")\n",
    "\n",
    "    rel_rep = section_report_path.relative_to(base_dir).as_posix()\n",
    "    section_block = f\"### Wk02_Section4\\n- [Week 2 – Section 4: Data Splitting & Preparation](./{rel_rep})\"\n",
    "\n",
    "    wk2_path = base_dir / WEEK_REPORT_FILENAME\n",
    "    if wk2_path.exists():\n",
    "        md = _ensure_toc_item(md, \"Top-level Week 2 Report\")\n",
    "        if \"## Top-level Week 2 Report\" not in md:\n",
    "            md += f\"\\n## Top-level Week 2 Report\\n\\n- [SDS-CP036-powercast_Wk02_Report_Business.md](./{wk2_path.relative_to(base_dir).as_posix()})\\n\"\n",
    "\n",
    "    md = _insert_at_end_of_section(md, \"Quick Gallery (click any thumbnail)\", thumbs_block)\n",
    "    md = _insert_at_end_of_section(md, \"Plots (grouped by Section)\", plots_block)\n",
    "    md = _insert_at_end_of_section(md, \"Section Reports (grouped)\", section_block)\n",
    "\n",
    "    readme.write_text(md, encoding=\"utf-8\")\n",
    "    return str(readme)\n",
    "\n",
    "# ---------------- Main process ----------------\n",
    "def process(base_dir: Path):\n",
    "    base_dir = Path(base_dir)\n",
    "    out_dir, features_dir, plots_dir, reports_dir = _setup_dirs(base_dir)\n",
    "    _clean_prev(features_dir, plots_dir, reports_dir)\n",
    "\n",
    "    mode, paths = _resolve_best_input(base_dir)\n",
    "    if mode == \"section3\":\n",
    "        train = pd.read_csv(paths[0]); test = pd.read_csv(paths[1])\n",
    "        dt_col = _find_datetime_column(train) or _find_datetime_column(test)\n",
    "        if dt_col and not (pd.api.types.is_datetime64_any_dtype(train[dt_col]) or pd.api.types.is_datetime64tz_dtype(train[dt_col])):\n",
    "            train[dt_col] = _ensure_dt(train[dt_col]); test[dt_col] = _ensure_dt(test[dt_col])\n",
    "        src_name = f\"{paths[0].name} + {paths[1].name}\"\n",
    "        df_all = pd.concat([train, test], ignore_index=True)\n",
    "    else:\n",
    "        df = pd.read_csv(paths[0])\n",
    "        dt_col = _find_datetime_column(df)\n",
    "        if dt_col is None: raise ValueError(\"No datetime-like column found.\")\n",
    "        df[dt_col] = _ensure_dt(df[dt_col])\n",
    "        df = df.sort_values(dt_col).reset_index(drop=True)\n",
    "        train, test = _time_split(df, dt_col, test_size=0.2)\n",
    "        src_name = paths[0].name\n",
    "        df_all = df\n",
    "\n",
    "    target = _choose_target(df_all, dt_col)\n",
    "    diagnostics = {\n",
    "        \"source_mode\": mode,\n",
    "        \"source\": src_name,\n",
    "        \"rows\": int(len(df_all)),\n",
    "        \"rows_train\": int(len(train)),\n",
    "        \"rows_test\": int(len(test)),\n",
    "        \"target\": target\n",
    "    }\n",
    "\n",
    "    train_csv = features_dir/\"train.csv\"; train.to_csv(train_csv, index=False)\n",
    "    test_csv  = features_dir/\"test.csv\";  test.to_csv(test_csv, index=False)\n",
    "\n",
    "    plot_path = _plot_time_coverage(df_all, dt_col, train, test, plots_dir)\n",
    "\n",
    "    # Walk-forward baseline using lag-1 on target (if available)\n",
    "    wf_csv = features_dir/\"walk_forward_results.csv\"\n",
    "    if target and target in df_all.columns:\n",
    "        folds = _walk_forward_folds(df_all[[dt_col, target]].copy(), dt_col, n_folds=3)\n",
    "        rows = []\n",
    "        for i, (tr, te) in enumerate(folds, 1):\n",
    "            y = pd.to_numeric(te[target], errors=\"coerce\")\n",
    "            # naive lag-1 using last seen value from train part to predict test segment\n",
    "            last_train_val = pd.to_numeric(tr[target], errors=\"coerce\").iloc[-1] if len(tr) else np.nan\n",
    "            pred = pd.Series(last_train_val, index=y.index)\n",
    "            rows.append({\"fold\": i, \"mae_lag1\": _mae(y.values, pred.values)})\n",
    "        pd.DataFrame(rows).to_csv(wf_csv, index=False)\n",
    "    else:\n",
    "        pd.DataFrame([{\"note\":\"no target available for baseline\"}]).to_csv(wf_csv, index=False)\n",
    "\n",
    "    # Business-friendly Q&A\n",
    "    qna = _business_qna_text()\n",
    "\n",
    "    # Section report\n",
    "    artifacts = {\"train\": str(train_csv), \"test\": str(test_csv), \"wf\": str(wf_csv)}\n",
    "    section_report = _write_section_report(reports_dir, src_name, diagnostics, qna, artifacts, plot_path)\n",
    "\n",
    "    # Summary\n",
    "    (out_dir/\"summary.json\").write_text(json.dumps({\"input_mode\": mode, **diagnostics}, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    # Week report block\n",
    "    block = []\n",
    "    block.append(\"## Section 4 — Data Splitting & Preparation\")\n",
    "    block.append(\"\")\n",
    "    block.append(\"### Key Questions Answered\")\n",
    "    block.append(\"Q: How did you split your data into training and test sets to maintain chronological order?\")\n",
    "    block.append(\"A: \" + qna[0])\n",
    "    block.append(\"\")\n",
    "    block.append(\"Q: What steps did you take to prevent information leakage between splits?\")\n",
    "    block.append(\"A: \" + qna[1])\n",
    "    block.append(\"\")\n",
    "    block.append(\"Q: How did you verify that your train/test split was appropriate for time-series forecasting?\")\n",
    "    block.append(\"A: \" + qna[2])\n",
    "    block.append(\"\")\n",
    "    if plot_path:\n",
    "        rel = Path(plot_path).relative_to(base_dir).as_posix()\n",
    "        block.append(f\"![Split marker]({rel})\")\n",
    "    week_report = _update_week_report(base_dir, \"\\n\".join(block))\n",
    "\n",
    "    # README\n",
    "    readme = _update_readme(base_dir, section_report, plot_path)\n",
    "\n",
    "    print(json.dumps({\n",
    "        \"train_csv\": str(train_csv),\n",
    "        \"test_csv\": str(test_csv),\n",
    "        \"walk_forward_csv\": str(wf_csv),\n",
    "        \"plot\": str(plot_path) if plot_path else None,\n",
    "        \"section_report\": str(section_report),\n",
    "        \"week_report\": week_report,\n",
    "        \"readme\": readme\n",
    "    }, indent=2))\n",
    "\n",
    "# ---------------- Execute ----------------\n",
    "BASE = find_base_dir(Path.cwd())\n",
    "process(BASE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a452f16d-baf3-444d-8e3e-4ab8883da6fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
