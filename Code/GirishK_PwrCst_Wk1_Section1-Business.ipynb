{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f456b843-360b-4759-8bee-376548b0e0ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T23:19:37.597038Z",
     "iopub.status.busy": "2025-08-15T23:19:37.596602Z",
     "iopub.status.idle": "2025-08-15T23:19:37.906029Z",
     "shell.execute_reply": "2025-08-15T23:19:37.905496Z",
     "shell.execute_reply.started": "2025-08-15T23:19:37.597017Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Section 1 (Business) complete. Outputs written to:\n",
      "- /home/6376f5a9-d12b-4255-9426-c0091ad440a7/Powercast/results/Wk01_Section1/reports/SDS-CP036-powercast_Wk01_Section1_Report_Business.md\n",
      "- /home/6376f5a9-d12b-4255-9426-c0091ad440a7/Powercast/results/Wk01_Section1/SDS-CP036-powercast_Wk01_Section1_Business_Report.md\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===== Week 1 ‚Äì Section 1: Time Consistency & Structure (Business) =====\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- Shared Header / Naming ----------\n",
    "BASE_PROJECT_NAME = \"SDS-CP036-powercast\"\n",
    "WEEK = \"Wk01\"\n",
    "SECTION = \"Section1\"\n",
    "RUN_TAG = f\"{WEEK}_{SECTION}\"\n",
    "\n",
    "# Prefer running relative to this file; fall back to CWD when executed in notebooks\n",
    "BASE_DIR = Path(__file__).resolve().parent if \"__file__\" in globals() else Path.cwd().resolve()\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    \"\"\"Find a reasonable project root; if not found, use start.\"\"\"\n",
    "    cur = start\n",
    "    for _ in range(10):\n",
    "        if (cur / \".git\").exists() or (cur / \"data\").exists():\n",
    "            return cur\n",
    "        if cur.parent == cur:\n",
    "            break\n",
    "        cur = cur.parent\n",
    "    return start\n",
    "\n",
    "REPO_ROOT = find_repo_root(BASE_DIR)\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "\n",
    "# --- Locate energy CSV flexibly ---\n",
    "CANDIDATES = [\n",
    "    DATA_DIR / \"Tetuan City power consumption.csv\",\n",
    "    Path(\"/mnt/data/Tetuan City power consumption.csv\"),\n",
    "]\n",
    "\n",
    "# Allow override via env var ENERGY_CSV_PATH\n",
    "env_path = os.environ.get(\"ENERGY_CSV_PATH\")\n",
    "if env_path:\n",
    "    CANDIDATES.insert(0, Path(env_path))\n",
    "\n",
    "ENERGY_CSV = next((p for p in CANDIDATES if p.exists()), None)\n",
    "if ENERGY_CSV is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not locate an energy CSV. Looked for:\\n  - \" + \"\\n  - \".join(map(str, CANDIDATES))\n",
    "    )\n",
    "\n",
    "RESULTS_DIR  = REPO_ROOT / \"results\" / RUN_TAG\n",
    "PLOTS_DIR    = RESULTS_DIR / \"plots\"\n",
    "REPORTS_DIR  = RESULTS_DIR / \"reports\"\n",
    "for d in (RESULTS_DIR, PLOTS_DIR, REPORTS_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BUSINESS_SUMMARY_MD  = REPORTS_DIR / f\"{BASE_PROJECT_NAME}_{RUN_TAG}_Report_Business.md\"\n",
    "BUSINESS_REPORT_MD   = RESULTS_DIR / f\"{BASE_PROJECT_NAME}_{RUN_TAG}_Business_Report.md\"\n",
    "\n",
    "# ---------- Load energy safely ----------\n",
    "def _parse_datetime_series(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Try parsing with month-first then day-first as fallback.\"\"\"\n",
    "    dt = pd.to_datetime(series, errors=\"coerce\")\n",
    "    # If too many NaT, try day-first\n",
    "    if dt.isna().mean() > 0.5:\n",
    "        dt = pd.to_datetime(series, errors=\"coerce\", dayfirst=True)\n",
    "    return dt\n",
    "\n",
    "def load_energy(path: Path) -> pd.DataFrame:\n",
    "    # Read with flexible delimiter\n",
    "    df = pd.read_csv(path, sep=',', low_memory=False)\n",
    "    if len(df.columns) == 1:\n",
    "        df = pd.read_csv(path, sep=';', low_memory=False)\n",
    "\n",
    "    # Normalize column names (strip spaces, preserve original for content)\n",
    "    original_cols = df.columns.tolist()\n",
    "    normalized = {c: c.strip() for c in df.columns}\n",
    "    df.rename(columns=normalized, inplace=True)\n",
    "\n",
    "    # Common datetime patterns across datasets:\n",
    "    #  1) Single 'DateTime' column\n",
    "    #  2) Separate 'Date' and 'Time' columns\n",
    "    #  3) A combined 'Date,Time' column (CSV with comma inside a field)\n",
    "    cols_lower = {c.lower(): c for c in df.columns}\n",
    "\n",
    "    if \"datetime\" in cols_lower:\n",
    "        dt_col = cols_lower[\"datetime\"]\n",
    "        df[\"DateTime\"] = _parse_datetime_series(df[dt_col].astype(str))\n",
    "    elif (\"date\" in cols_lower) and (\"time\" in cols_lower):\n",
    "        c_date, c_time = cols_lower[\"date\"], cols_lower[\"time\"]\n",
    "        df[\"DateTime\"] = _parse_datetime_series(df[c_date].astype(str) + \" \" + df[c_time].astype(str))\n",
    "    elif \"date,time\" in df.columns:\n",
    "        # Some exports may keep 'Date,Time' literally as a header\n",
    "        dt = df[\"Date,Time\"].astype(str).str.split(\",\", n=1, expand=True)\n",
    "        dt.columns = [\"_Date_tmp\", \"_Time_tmp\"]\n",
    "        df[\"DateTime\"] = _parse_datetime_series(dt[\"_Date_tmp\"] + \" \" + dt[\"_Time_tmp\"])\n",
    "    else:\n",
    "        # Last resort: find the first column that *looks* like a datetime\n",
    "        candidate = None\n",
    "        for c in df.columns:\n",
    "            lc = c.lower()\n",
    "            if \"date\" in lc or \"time\" in lc or \"timestamp\" in lc:\n",
    "                candidate = c\n",
    "                break\n",
    "        if candidate is not None:\n",
    "            df[\"DateTime\"] = _parse_datetime_series(df[candidate].astype(str))\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Missing Date/Time information. Expected a 'DateTime' column or 'Date'+'Time'.\\n\"\n",
    "                f\"Columns found: {original_cols}\"\n",
    "            )\n",
    "\n",
    "    # Clean and sort\n",
    "    df = df.dropna(subset=[\"DateTime\"]).sort_values(\"DateTime\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "df = load_energy(ENERGY_CSV)\n",
    "\n",
    "# ---------- Analyses ----------\n",
    "# 1) Missing or irregular timestamps\n",
    "missing_timestamps = df[\"DateTime\"].isna().sum()\n",
    "\n",
    "df_sorted = df.sort_values(\"DateTime\").reset_index(drop=True)\n",
    "time_diffs = df_sorted[\"DateTime\"].diff()\n",
    "\n",
    "# Handle empty diffs safely\n",
    "if time_diffs.dropna().empty:\n",
    "    most_common = pd.Timedelta(0)\n",
    "else:\n",
    "    freq_counts = time_diffs.value_counts()\n",
    "    most_common = freq_counts.index[0] if not freq_counts.empty else pd.Timedelta(0)\n",
    "\n",
    "# 2) Duplicates\n",
    "duplicate_dt = df_sorted[\"DateTime\"].duplicated().sum()\n",
    "\n",
    "# ---------- Business Summary ----------\n",
    "def _timedelta_to_minutes(td):\n",
    "    try:\n",
    "        return int(td.total_seconds() // 60)\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "most_common_minutes = _timedelta_to_minutes(most_common)\n",
    "\n",
    "biz_md = f\"\"\"# üíº Week 1 ‚Äì {SECTION}: Time Consistency & Structure (Business-Friendly Report)\n",
    "\n",
    "## Dataset\n",
    "Using file: **{ENERGY_CSV.name}**\n",
    "\n",
    "## Key Questions Answered\n",
    "**Q1: Are there any missing or irregular timestamps in the dataset? How did you verify consistency?**  \n",
    "I created a canonical `DateTime` column and inspected gaps between consecutive records to detect irregularities.\n",
    "\n",
    "**Q2: What is the sampling frequency and are all records spaced consistently?**  \n",
    "I measured the time deltas between consecutive rows. The most common spacing is **{most_common}**, suggesting the intended sampling cadence.\n",
    "\n",
    "**Q3: Did you encounter any duplicates or inconsistent `DateTime` entries?**  \n",
    "I found **{duplicate_dt}** duplicate timestamps (exact same `DateTime`). These could be reviewed or deduplicated based on your business rules.\n",
    "\n",
    "## Plain-English Notes\n",
    "- Built a single `DateTime` column from whatever the dataset provided (flexible parsing with comma/semicolon delimiters and both month-first/day-first formats).\n",
    "- Looked for missing times and uneven gaps using the distribution of time differences.\n",
    "- Business takeaway: ‚ÄúOn average, there‚Äôs a reading roughly every {most_common} ‚Äî i.e., about {most_common_minutes} minute(s) per record.‚Äù\n",
    "\"\"\"\n",
    "\n",
    "BUSINESS_SUMMARY_MD.write_text(biz_md, encoding=\"utf-8\")\n",
    "\n",
    "wrapper = f\"\"\"# {BASE_PROJECT_NAME} ‚Äî {RUN_TAG} ‚Äî Business Report\n",
    "\n",
    "üîó **Open Business Summary:** `{BUSINESS_SUMMARY_MD.name}`\n",
    "\"\"\"\n",
    "BUSINESS_REPORT_MD.write_text(wrapper, encoding=\"utf-8\")\n",
    "\n",
    "print(\"‚úÖ Section 1 (Business) complete. Outputs written to:\")\n",
    "print(\"-\", BUSINESS_SUMMARY_MD)\n",
    "print(\"-\", BUSINESS_REPORT_MD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f39f6c7-e8fd-40cb-930b-726d736a41da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
