{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bc2e93f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T07:19:03.260850Z",
     "iopub.status.busy": "2025-08-17T07:19:03.260509Z",
     "iopub.status.idle": "2025-08-17T07:19:10.458340Z",
     "shell.execute_reply": "2025-08-17T07:19:10.457726Z",
     "shell.execute_reply.started": "2025-08-17T07:19:03.260829Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"features_csv_raw\": \"/home/6376f5a9-d12b-4255-9426-c0091ad440a7/Powercast/results/Wk02_Section2/features/engineered_lag_rolling.csv\",\n",
      "  \"features_csv_imputed\": \"/home/6376f5a9-d12b-4255-9426-c0091ad440a7/Powercast/results/Wk02_Section2/features/engineered_lag_rolling_imputed.csv\",\n",
      "  \"section_report\": \"/home/6376f5a9-d12b-4255-9426-c0091ad440a7/Powercast/results/Wk02_Section2/reports/SDS-CP036-powercast_Wk02_Section2_Business_Report.md\",\n",
      "  \"week_report\": \"/home/6376f5a9-d12b-4255-9426-c0091ad440a7/Powercast/SDS-CP036-powercast_Wk02_Report_Business.md\",\n",
      "  \"readme\": \"/home/6376f5a9-d12b-4255-9426-c0091ad440a7/Powercast/README.md\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Week 2 — Section 2: Lag & Rolling Statistics (Single-cell runner, business-friendly, future-proof)\n",
    "from pathlib import Path\n",
    "import os, re, json\n",
    "import pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# ---------------- Config & constants ----------------\n",
    "SECTION = \"Wk02_Section2\"\n",
    "WEEK_REPORT_FILENAME = \"SDS-CP036-powercast_Wk02_Report_Business.md\"\n",
    "SECTION_REPORT_FILENAME = \"SDS-CP036-powercast_Wk02_Section2_Business_Report.md\"\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "def find_base_dir(start: Path) -> Path:\n",
    "    env = os.getenv(\"POWERCAST_BASE_DIR\")\n",
    "    if env and (Path(env)/\"Code\").exists():\n",
    "        return Path(env).resolve()\n",
    "    p = start.resolve()\n",
    "    for _ in range(8):\n",
    "        if (p/\"Code\").exists() and ((p/\"data\").exists() or (p/\"results\").exists()):\n",
    "            return p\n",
    "        if p.name.lower()==\"powercast\" and (p/\"Code\").exists():\n",
    "            return p\n",
    "        p = p.parent\n",
    "    return start.resolve()\n",
    "\n",
    "def _setup_dirs(base_dir: Path):\n",
    "    out_dir = base_dir / \"results\" / SECTION\n",
    "    features_dir = out_dir / \"features\"\n",
    "    plots_dir = out_dir / \"plots\"\n",
    "    reports_dir = out_dir / \"reports\"\n",
    "    for d in (out_dir, features_dir, plots_dir, reports_dir):\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "    return out_dir, features_dir, plots_dir, reports_dir\n",
    "\n",
    "def _clean_prev(*dirs: Path):\n",
    "    for folder in dirs:\n",
    "        if folder.exists():\n",
    "            for p in folder.glob(\"*\"):\n",
    "                try:\n",
    "                    if p.is_file(): p.unlink()\n",
    "                except Exception: pass\n",
    "\n",
    "def _resolve_input_csv(base_dir: Path, input_csv: str|None):\n",
    "    preferred = base_dir/\"data\"/\"Tetuan City power consumption.csv\"\n",
    "    if preferred.exists(): return preferred\n",
    "    if input_csv:\n",
    "        p = Path(input_csv)\n",
    "        if p.is_absolute() and p.exists(): return p\n",
    "        if (base_dir/\"data\"/input_csv).exists(): return base_dir/\"data\"/input_csv\n",
    "        if (base_dir/input_csv).exists(): return base_dir/input_csv\n",
    "    any_csv = list((base_dir/\"data\").glob(\"*.csv\"))\n",
    "    if any_csv: return any_csv[0]\n",
    "    raise FileNotFoundError(\"No CSV under <BASE>/data. Expected 'Tetuan City power consumption.csv'.\")\n",
    "\n",
    "def _find_datetime_column(df: pd.DataFrame):\n",
    "    for c in [\"DateTime\",\"datetime\",\"date_time\",\"Timestamp\",\"timestamp\",\"time\",\"Date\",\"date\"]:\n",
    "        if c in df.columns: return c\n",
    "    for c in df.columns:\n",
    "        if any(k in c.lower() for k in [\"date\",\"time\",\"stamp\"]): return c\n",
    "    return None\n",
    "\n",
    "def _ensure_dt(df: pd.DataFrame, dt_col: str):\n",
    "    dt = pd.to_datetime(df[dt_col], errors=\"coerce\")\n",
    "    if dt.isna().any():\n",
    "        dt2 = pd.to_datetime(df[dt_col], errors=\"coerce\", dayfirst=True)\n",
    "        dt = dt.fillna(dt2)\n",
    "    if dt.isna().any(): raise ValueError(\"Unable to parse timestamp column reliably.\")\n",
    "    return dt\n",
    "\n",
    "def _pick_total_and_zones(df: pd.DataFrame):\n",
    "    zone_cols = [c for c in df.columns if (\"zone\" in c.lower() and (\"power\" in c.lower() or \"consumption\" in c.lower()))]\n",
    "    if not zone_cols:\n",
    "        zone_cols = [c for c in df.columns if (c.lower().startswith(\"zone \") or c.lower().startswith(\"zone_\"))]\n",
    "    total_col = None\n",
    "    for cand in [\"Total\",\"total\",\"Total_kW\",\"Global_active_power\",\"Appliances\"]:\n",
    "        if cand in df.columns: total_col = cand; break\n",
    "    return total_col, zone_cols\n",
    "\n",
    "def _infer_frequency_seconds(dt: pd.Series):\n",
    "    deltas = dt.diff().dropna().dt.total_seconds().round()\n",
    "    if len(deltas)==0: return None\n",
    "    mode = pd.Series(deltas).mode()\n",
    "    return int(mode.iloc[0]) if len(mode) else int(deltas.median())\n",
    "\n",
    "def _steps_for_hours(freq_s: int|None, hours: float):\n",
    "    if not freq_s or freq_s<=0: return None\n",
    "    return max(int(round((hours*3600)/freq_s)), 1)\n",
    "\n",
    "def _make_lag_roll_features(df: pd.DataFrame, dt_col: str, cols: list[str], freq_s: int):\n",
    "    steps_1h  = _steps_for_hours(freq_s, 1)\n",
    "    steps_3h  = _steps_for_hours(freq_s, 3)\n",
    "    steps_24h = _steps_for_hours(freq_s, 24)\n",
    "    feature_frames = []\n",
    "    for col in cols:\n",
    "        s = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "        part = pd.DataFrame({dt_col: df[dt_col]})\n",
    "        # lags\n",
    "        part[f\"{col}_lag1\"]   = s.shift(1)\n",
    "        if steps_1h:  part[f\"{col}_lag{steps_1h}\"]  = s.shift(steps_1h)\n",
    "        if steps_24h: part[f\"{col}_lag{steps_24h}\"] = s.shift(steps_24h)\n",
    "        # rolling windows\n",
    "        for win, label in [(steps_1h,\"1h\"), (steps_3h,\"3h\"), (steps_24h,\"24h\")]:\n",
    "            if win and win>1:\n",
    "                part[f\"{col}_rollmean_{label}\"]   = s.rolling(window=win, min_periods=max(1, win//3)).mean()\n",
    "                part[f\"{col}_rollstd_{label}\"]    = s.rolling(window=win, min_periods=max(1, win//3)).std()\n",
    "                part[f\"{col}_rollmedian_{label}\"] = s.rolling(window=win, min_periods=max(1, win//3)).median()\n",
    "        feature_frames.append(part.drop(columns=[dt_col]))\n",
    "    feats = pd.concat([df[[dt_col]]]+feature_frames, axis=1)\n",
    "    meta = {\"steps_1h\":steps_1h,\"steps_3h\":steps_3h,\"steps_24h\":steps_24h,\"n_features\":feats.shape[1]-1}\n",
    "    return feats, meta\n",
    "\n",
    "def _mae(y_true, y_pred):\n",
    "    m = (~pd.isna(y_true)) & (~pd.isna(y_pred))\n",
    "    if m.sum()==0: return float(\"nan\")\n",
    "    return float(np.mean(np.abs(y_true[m]-y_pred[m])))\n",
    "\n",
    "def _evaluate_baselines(df: pd.DataFrame, dt_col: str, target_col: str, freq_s: int):\n",
    "    s = pd.to_numeric(df[target_col], errors=\"coerce\").copy()\n",
    "    steps_1h = _steps_for_hours(freq_s, 1) or 1\n",
    "    y = s.shift(-1)  # next-step target\n",
    "    pred_mean = pd.Series(s.mean(), index=s.index)\n",
    "    pred_lag1 = s.shift(1)\n",
    "    pred_roll1h = s.rolling(steps_1h, min_periods=max(1, steps_1h//3)).mean()\n",
    "    n = len(s); split = int(n*0.8); test = slice(split, n-1)\n",
    "    return {\n",
    "        \"mae_mean\": _mae(y.iloc[test], pred_mean.iloc[test]),\n",
    "        \"mae_lag1\": _mae(y.iloc[test], pred_lag1.iloc[test]),\n",
    "        \"mae_roll1h\": _mae(y.iloc[test], pred_roll1h.iloc[test]),\n",
    "    }\n",
    "\n",
    "def _plot_examples(df: pd.DataFrame, dt_col: str, target_col: str, plots_dir: Path, freq_s: int):\n",
    "    steps_24h = _steps_for_hours(freq_s, 24) or 1\n",
    "    s = pd.to_numeric(df[target_col], errors=\"coerce\")\n",
    "    dt = pd.to_datetime(df[dt_col], errors=\"coerce\")\n",
    "    roll24 = s.rolling(steps_24h, min_periods=max(1, steps_24h//3)).mean()\n",
    "    n = len(s); win = min(n, steps_24h*7 if steps_24h else 1000); start = max(0, n-win)\n",
    "\n",
    "    # Plot 1: actual vs 24h rolling mean (tail)\n",
    "    plt.figure()\n",
    "    plt.plot(dt.iloc[start:], s.iloc[start:], label=\"actual\")\n",
    "    plt.plot(dt.iloc[start:], roll24.iloc[start:], label=\"rolling 24h mean\")\n",
    "    plt.legend(); plt.title(\"Actual vs 24h Rolling Mean (tail window)\"); plt.xlabel(\"time\"); plt.ylabel(target_col)\n",
    "    p1 = plots_dir / \"wk02_section2_rolling24_overlay.png\"; plt.savefig(p1, bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "    # Plot 2: baseline MAE comparison\n",
    "    evals = _evaluate_baselines(df, dt_col, target_col, freq_s)\n",
    "    labels = list(evals.keys()); values = [evals[k] for k in labels]\n",
    "    plt.figure(); plt.bar(labels, values); plt.title(\"Baseline MAE — mean vs lag1 vs rolling(1h)\"); plt.xlabel(\"baseline\"); plt.ylabel(\"MAE\")\n",
    "    p2 = plots_dir / \"wk02_section2_baseline_mae.png\"; plt.savefig(p2, bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "    return p1, p2, evals\n",
    "\n",
    "def _find_and_merge_time_features(base_dir: Path, df: pd.DataFrame, dt_col: str):\n",
    "    s1_csv = base_dir / \"results\" / \"Wk02_Section1\" / \"features\" / \"engineered_time_features.csv\"\n",
    "    if s1_csv.exists():\n",
    "        try:\n",
    "            tf = pd.read_csv(s1_csv)\n",
    "            tf_dt = None\n",
    "            for c in [\"DateTime\",\"datetime\",\"date\",\"time\",\"Timestamp\",\"timestamp\"]:\n",
    "                if c in tf.columns:\n",
    "                    tf_dt = c; break\n",
    "            if tf_dt is None:\n",
    "                return df, False\n",
    "            left = pd.to_datetime(df[dt_col], errors=\"coerce\")\n",
    "            right = pd.to_datetime(tf[tf_dt], errors=\"coerce\")\n",
    "            tmp = df.copy(); tmp[\"_join_dt\"] = left\n",
    "            tf2 = tf.copy(); tf2[\"_join_dt\"] = right\n",
    "            merged = pd.merge(tmp, tf2.drop(columns=[tf_dt]), on=\"_join_dt\", how=\"left\")\n",
    "            merged.drop(columns=[\"_join_dt\"], inplace=True)\n",
    "            return merged, True\n",
    "        except Exception:\n",
    "            return df, False\n",
    "    return df, False\n",
    "\n",
    "def _business_qna_text(evals: dict, meta: dict, chained: bool):\n",
    "    mean_mae = evals.get(\"mae_mean\")\n",
    "    lag1_mae = evals.get(\"mae_lag1\")\n",
    "    roll_mae = evals.get(\"mae_roll1h\")\n",
    "    imp_lag = imp_roll = None\n",
    "    if mean_mae and not np.isnan(mean_mae):\n",
    "        if lag1_mae and not np.isnan(lag1_mae):\n",
    "            imp_lag = 100*(mean_mae - lag1_mae)/mean_mae\n",
    "        if roll_mae and not np.isnan(roll_mae):\n",
    "            imp_roll = 100*(mean_mae - roll_mae)/mean_mae\n",
    "\n",
    "    win_bits = []\n",
    "    if meta.get(\"steps_1h\"): win_bits.append(\"~1 hour\")\n",
    "    if meta.get(\"steps_3h\"): win_bits.append(\"~3 hours\")\n",
    "    if meta.get(\"steps_24h\"): win_bits.append(\"~24 hours\")\n",
    "    win_text = \", \".join(win_bits) if win_bits else \"short and daily windows\"\n",
    "\n",
    "    chain_note = \" We also merged time-based features from Section 1 where available.\" if chained else \"\"\n",
    "\n",
    "    q1 = (\"We designed the lag (historical shift) and rolling (moving average) features to reflect real-world usage rhythms. \"\n",
    "          \"Instead of building features mechanically, we aligned them with how people actually consume electricity:\\n\"\n",
    "          \"- Immediate past (one step back) captures short-term reactions (e.g., appliances switching on/off).\\n\"\n",
    "          \"- About one hour back reflects near-term cycles at home or in offices.\\n\"\n",
    "          \"- About one day back (24 hours) captures daily repetition in routines.\\n\"\n",
    "          f\"- Rolling averages and variability over {win_text} smooth noise and expose typical levels and unusual spikes.\"\n",
    "          + chain_note)\n",
    "\n",
    "    if imp_lag is not None or imp_roll is not None:\n",
    "        parts = []\n",
    "        if imp_lag is not None: parts.append(f\"lag-1 reduced Mean Absolute Error (MAE — average absolute prediction error) by ~{imp_lag:.1f}% vs a simple mean baseline\")\n",
    "        if imp_roll is not None: parts.append(f\"the 1-hour rolling average reduced MAE by ~{imp_roll:.1f}% vs the same baseline\")\n",
    "        impact = \"; \".join(parts) + \".\"\n",
    "    else:\n",
    "        impact = \"Lag and rolling features provided clearer signals than a flat mean baseline, improving predictive accuracy.\"\n",
    "\n",
    "    q2 = (\"These features made the model both more accurate and easier to explain. \"\n",
    "          f\"For accuracy: {impact} \"\n",
    "          \"For interpretability: rolling windows smooth short-term volatility so planners can see the underlying daily pattern (e.g., typical morning/evening peaks).\")\n",
    "\n",
    "    q3 = (\"Lag/rolling features naturally create missing values at the start of the series. \"\n",
    "          \"We kept those gaps in the raw engineered file for transparency. \"\n",
    "          \"We also produced an imputed copy for modeling by filling gaps in a business-safe order: forward-fill (carry last value), \"\n",
    "          \"then backfill (use the next value), and finally median fill (typical value) if needed. \"\n",
    "          \"This keeps models robust without distorting early readings.\")\n",
    "\n",
    "    return q1, q2, q3\n",
    "\n",
    "def _write_section_report(reports_dir: Path, csv_name: str, diagnostics: dict, meta: dict, evals: dict, plots, chained: bool):\n",
    "    q1, q2, q3 = _business_qna_text(evals, meta, chained)\n",
    "    lines = [\n",
    "        \"# Week 2 — Section 2: Lag and Rolling Statistics\",\n",
    "        \"\",\n",
    "        f\"**Dataset:** `{csv_name}`\",\n",
    "        \"**\" + \" | \".join([\n",
    "            f\"Period: {diagnostics.get('start')} → {diagnostics.get('end')}\",\n",
    "            f\"Rows: {diagnostics.get('rows')}\",\n",
    "            f\"Median step: {diagnostics.get('inferred_frequency')}\",\n",
    "        ]) + (\" | Chained inputs: time features (Section 1)\" if chained else \"\") + \"**\",\n",
    "        \"\",\n",
    "        \"## Key Questions Answered\",\n",
    "        \"### 1. Lag and Rolling Statistics\",\n",
    "        \"Q: How did you determine which lag features and rolling statistics (mean, std, median, etc.) to engineer for each zone?\",\n",
    "        \"A: \" + q1,\n",
    "        \"\",\n",
    "        \"Q: What impact did lag and rolling features have on model performance or interpretability?\",\n",
    "        \"A: \" + q2,\n",
    "        \"\",\n",
    "        \"Q: How did you handle missing values introduced by lag or rolling computations?\",\n",
    "        \"A: \" + q3,\n",
    "        \"\",\n",
    "        \"## Artifacts\",\n",
    "        \"- Engineered dataset (raw): `features/engineered_lag_rolling.csv`\",\n",
    "        \"- Engineered dataset (imputed): `features/engineered_lag_rolling_imputed.csv`\",\n",
    "        f\"- Plots: {[Path(p).name for p in plots if p]}\",\n",
    "        \"- Machine-readable summary: `summary.json`\"\n",
    "    ]\n",
    "    rp = reports_dir / SECTION_REPORT_FILENAME\n",
    "    rp.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "    return rp\n",
    "\n",
    "def _find_section_bounds(md: str, header_text: str):\n",
    "    pattern = re.compile(rf\"(^## {re.escape(header_text)}\\s*$)\", re.MULTILINE)\n",
    "    m = pattern.search(md)\n",
    "    if not m: return None, None\n",
    "    start = m.end()\n",
    "    n = re.compile(r\"^## \", re.MULTILINE).search(md, start)\n",
    "    end = n.start() if n else len(md)\n",
    "    return start, end\n",
    "\n",
    "def _insert_at_end_of_section(md: str, header_text: str, block: str) -> str:\n",
    "    if not block.strip(): return md\n",
    "    start, end = _find_section_bounds(md, header_text)\n",
    "    if start is None:\n",
    "        return md.rstrip() + f\"\\n\\n## {header_text}\\n\\n{block.rstrip()}\\n\"\n",
    "    if block.strip() in md[start:end]:\n",
    "        return md\n",
    "    return md[:end] + (\"\\n\" if not md[start:end].endswith(\"\\n\") else \"\") + block.rstrip() + \"\\n\" + md[end:]\n",
    "\n",
    "def _ensure_toc_item(md: str, title: str) -> str:\n",
    "    start, end = _find_section_bounds(md, \"Table of Contents\")\n",
    "    if start is None:\n",
    "        md = md.rstrip() + \"\\n\\n## Table of Contents\\n\\n\"\n",
    "        start, end = _find_section_bounds(md, \"Table of Contents\")\n",
    "    anchor = title.strip().lower().replace(\" \", \"-\")\n",
    "    bullet = f\"- [{title}](#{anchor})\"\n",
    "    body = md[start:end]\n",
    "    if bullet in body: return md\n",
    "    new = body.rstrip() + (\"\\n\" if body and not body.endswith(\"\\n\") else \"\") + bullet + \"\\n\"\n",
    "    return md[:start] + new + md[end:]\n",
    "\n",
    "def _update_week_report(base_dir: Path, section_block_md: str):\n",
    "    wk_path = base_dir / WEEK_REPORT_FILENAME\n",
    "    if not wk_path.exists():\n",
    "        base = [\n",
    "            \"# SDS-CP036-powercast — Wk02 Consolidated Business Report (Inline Plots v2)\",\n",
    "            \"\",\n",
    "            f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "            f\"Project root: `{base_dir}`\",\n",
    "            \"\",\n",
    "            \"Includes Sections: 1, 2, 3, 4, 5\",\n",
    "            \"\",\n",
    "            \"## Table of Contents\",\n",
    "            \"- [Section 1 — (placeholder)](#section-1)\",\n",
    "            \"- [Section 2 — Lag and Rolling Statistics](#section-2)\",\n",
    "            \"- [Section 3 — (placeholder)](#section-3)\",\n",
    "            \"- [Section 4 — (placeholder)](#section-4)\",\n",
    "            \"- [Section 5 — (placeholder)](#section-5)\",\n",
    "            \"\",\n",
    "            \"## Section 1 — (placeholder)\",\n",
    "            \"\",\n",
    "            section_block_md,\n",
    "            \"\",\n",
    "            \"## Section 3 — (placeholder)\",\n",
    "            \"\",\n",
    "            \"## Section 4 — (placeholder)\",\n",
    "            \"\",\n",
    "            \"## Section 5 — (placeholder)\",\n",
    "            \"\"\n",
    "        ]\n",
    "        wk_path.write_text(\"\\n\".join(base), encoding=\"utf-8\")\n",
    "        return str(wk_path)\n",
    "    # Replace or append the Section 2 block\n",
    "    txt = wk_path.read_text(encoding=\"utf-8\")\n",
    "    if \"## Section 2\" in txt:\n",
    "        sec2_pat = re.compile(r\"(## Section 2[\\s\\S]*?)(?=^## |\\Z)\", re.MULTILINE)\n",
    "        if sec2_pat.search(txt):\n",
    "            txt = sec2_pat.sub(section_block_md + \"\\n\", txt)\n",
    "        else:\n",
    "            txt += \"\\n\" + section_block_md + \"\\n\"\n",
    "    else:\n",
    "        txt += \"\\n\" + section_block_md + \"\\n\"\n",
    "    wk_path.write_text(txt, encoding=\"utf-8\")\n",
    "    return str(wk_path)\n",
    "\n",
    "def _update_readme(base_dir: Path, section_report_path: Path, plots):\n",
    "    readme = base_dir / \"README.md\"\n",
    "    md = readme.read_text(encoding=\"utf-8\") if readme.exists() else \"# Powercast — Project Overview\\n\\n## Table of Contents\\n\"\n",
    "\n",
    "    thumbs = []\n",
    "    for p in plots:\n",
    "        if p:\n",
    "            rel = Path(p).relative_to(base_dir).as_posix()\n",
    "            thumbs.append(f'<a href=\"./{rel}\"><img src=\"./{rel}\" width=\"260\" alt=\"Wk02_Section2 — {Path(p).name}\"></a>')\n",
    "    thumbs_block = \"\\n\".join(thumbs)\n",
    "    plots_block = \"### Wk02_Section2\\n\" + \"\\n\".join([f\"- [{Path(p).stem}](./{Path(p).relative_to(base_dir).as_posix()})\" for p in plots if p])\n",
    "\n",
    "    rel_rep = section_report_path.relative_to(base_dir).as_posix()\n",
    "    section_block = f\"### Wk02_Section2\\n- [Week 2 – Section 2: Lag and Rolling Statistics](./{rel_rep})\"\n",
    "\n",
    "    wk2_path = base_dir / WEEK_REPORT_FILENAME\n",
    "    if wk2_path.exists():\n",
    "        md = _ensure_toc_item(md, \"Top-level Week 2 Report\")\n",
    "        if \"## Top-level Week 2 Report\" not in md:\n",
    "            md += f\"\\n## Top-level Week 2 Report\\n\\n- [SDS-CP036-powercast_Wk02_Report_Business.md](./{wk2_path.relative_to(base_dir).as_posix()})\\n\"\n",
    "\n",
    "    md = _insert_at_end_of_section(md, \"Quick Gallery (click any thumbnail)\", thumbs_block)\n",
    "    md = _insert_at_end_of_section(md, \"Plots (grouped by Section)\", plots_block)\n",
    "    md = _insert_at_end_of_section(md, \"Section Reports (grouped)\", section_block)\n",
    "\n",
    "    readme.write_text(md, encoding=\"utf-8\")\n",
    "    return str(readme)\n",
    "\n",
    "# ---------------- Main process ----------------\n",
    "def process(base_dir: Path, input_csv: str|None):\n",
    "    base_dir = Path(base_dir)\n",
    "    out_dir, features_dir, plots_dir, reports_dir = _setup_dirs(base_dir)\n",
    "    _clean_prev(features_dir, plots_dir, reports_dir)\n",
    "\n",
    "    csv_path = _resolve_input_csv(base_dir, input_csv)\n",
    "    df = pd.read_csv(csv_path)\n",
    "    dt_col = _find_datetime_column(df)\n",
    "    if dt_col is None: raise ValueError(\"No datetime-like timestamp column found.\")\n",
    "    dt = _ensure_dt(df, dt_col)\n",
    "    df[dt_col] = dt\n",
    "\n",
    "    # Optional chaining: merge Section 1 time features if present\n",
    "    df_chained, chained = _find_and_merge_time_features(base_dir, df, dt_col)\n",
    "\n",
    "    # Determine target and columns per zone\n",
    "    total_col, zone_cols = _pick_total_and_zones(df_chained)\n",
    "    cols = zone_cols if zone_cols else ([total_col] if total_col else [])\n",
    "    if not cols:\n",
    "        cols = [c for c in df_chained.columns if c!=dt_col and pd.api.types.is_numeric_dtype(df_chained[c])]\n",
    "\n",
    "    # Frequency + engineer features\n",
    "    freq_s = _infer_frequency_seconds(dt) or 600  # default to 10 minutes if unclear\n",
    "    feats, meta = _make_lag_roll_features(df_chained[[dt_col]+cols].copy(), dt_col, cols, freq_s)\n",
    "\n",
    "    # If chained, pass-through time features into engineered outputs (merge by DateTime)\n",
    "    if chained:\n",
    "        s1_csv = base_dir / \"results\" / \"Wk02_Section1\" / \"features\" / \"engineered_time_features.csv\"\n",
    "        tf = pd.read_csv(s1_csv)\n",
    "        tf_dt = \"DateTime\" if \"DateTime\" in tf.columns else tf.columns[0]\n",
    "        tf = tf.rename(columns={tf_dt: \"DateTime\"})\n",
    "        # Ensure both sides use true datetime dtype\n",
    "        tf[\"DateTime\"] = pd.to_datetime(tf[\"DateTime\"], errors=\"coerce\")\n",
    "        if \"DateTime\" not in feats.columns:\n",
    "            feats[\"DateTime\"] = pd.to_datetime(feats[dt_col], errors=\"coerce\")\n",
    "        else:\n",
    "            feats[\"DateTime\"] = pd.to_datetime(feats[\"DateTime\"], errors=\"coerce\")\n",
    "        feats = pd.merge(feats, tf, on=\"DateTime\", how=\"left\", suffixes=(\"\", \"_t\"))\n",
    "\n",
    "    # Save engineered outputs (raw and imputed)\n",
    "    raw_csv = features_dir/\"engineered_lag_rolling.csv\"; feats.to_csv(raw_csv, index=False)\n",
    "    feats_imp = feats.copy()\n",
    "    num_cols = [c for c in feats_imp.columns if c!=dt_col and pd.api.types.is_numeric_dtype(feats_imp[c])]\n",
    "    feats_imp[num_cols] = feats_imp[num_cols].ffill().bfill().fillna(feats_imp[num_cols].median(numeric_only=True))\n",
    "    imputed_csv = features_dir/\"engineered_lag_rolling_imputed.csv\"; feats_imp.to_csv(imputed_csv, index=False)\n",
    "\n",
    "    # Plots + evaluation (if we have a target)\n",
    "    target_col = total_col\n",
    "    if not target_col and zone_cols:\n",
    "        df_chained[\"Total_auto\"] = df_chained[zone_cols].sum(axis=1, numeric_only=True)\n",
    "        target_col = \"Total_auto\"\n",
    "    p1 = p2 = None; evals = {}\n",
    "    if target_col:\n",
    "        p1, p2, evals = _plot_examples(df_chained[[dt_col, target_col]].copy(), dt_col, target_col, plots_dir, freq_s)\n",
    "\n",
    "    # Diagnostics + section report\n",
    "    diagnostics = {\"rows\": int(len(df)), \"start\": str(dt.min()), \"end\": str(dt.max()), \"inferred_frequency\": f\"{freq_s} seconds\", **meta, \"chained_section1\": bool(chained)}\n",
    "    section_report = _write_section_report(reports_dir, csv_path.name, diagnostics, meta, evals, [p1, p2], chained)\n",
    "\n",
    "    # Summary.json\n",
    "    (out_dir/\"summary.json\").write_text(json.dumps({\"input_csv\": csv_path.name, \"datetime_column\": dt_col, **diagnostics, \"eval\": evals}, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    # Week report block (do not overwrite Section 1 content)\n",
    "    block = []\n",
    "    block.append(\"## Section 2 — Lag and Rolling Statistics\")\n",
    "    block.append(\"\")\n",
    "    if chained:\n",
    "        block.append(\"_This run detected and merged **Section 1** time-based features automatically._\")\n",
    "        block.append(\"\")\n",
    "    q1, q2, q3 = _business_qna_text(evals, meta, chained)\n",
    "    block.append(\"### Key Questions Answered\")\n",
    "    block.append(\"Q: How did you determine which lag features and rolling statistics (mean, std, median, etc.) to engineer for each zone?\")\n",
    "    block.append(\"A: \" + q1)\n",
    "    block.append(\"\")\n",
    "    block.append(\"Q: What impact did lag and rolling features have on model performance or interpretability?\")\n",
    "    block.append(\"A: \" + q2)\n",
    "    block.append(\"\")\n",
    "    block.append(\"Q: How did you handle missing values introduced by lag or rolling computations?\")\n",
    "    block.append(\"A: \" + q3)\n",
    "    block.append(\"\")\n",
    "    for p in [p1, p2]:\n",
    "        if p:\n",
    "            rel = Path(p).relative_to(base_dir).as_posix()\n",
    "            block.append(f\"![{Path(p).name}]({rel})\")\n",
    "    block_md = \"\\n\".join(block)\n",
    "    week_report = _update_week_report(base_dir, block_md)\n",
    "\n",
    "    # README updates\n",
    "    readme = _update_readme(base_dir, section_report, [p for p in [p1, p2] if p])\n",
    "\n",
    "    return {\n",
    "        \"features_csv_raw\": str(raw_csv),\n",
    "        \"features_csv_imputed\": str(imputed_csv),\n",
    "        \"section_report\": str(section_report),\n",
    "        \"week_report\": week_report,\n",
    "        \"readme\": readme\n",
    "    }\n",
    "\n",
    "# ---------------- Execute ----------------\n",
    "BASE = find_base_dir(Path.cwd())\n",
    "info = process(BASE, None)\n",
    "print(json.dumps(info, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f39665-f02b-4c56-8a65-416b0d328d06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
