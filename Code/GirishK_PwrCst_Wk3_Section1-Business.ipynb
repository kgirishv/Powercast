{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3d6b652",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T07:17:28.145757Z",
     "iopub.status.busy": "2025-08-22T07:17:28.145194Z",
     "iopub.status.idle": "2025-08-22T07:19:35.979218Z",
     "shell.execute_reply": "2025-08-22T07:19:35.978615Z",
     "shell.execute_reply.started": "2025-08-22T07:17:28.145696Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:18:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "07:18:18 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section report path: /home/6376f5a9-d12b-4255-9426-c0091ad440a7/Powercast/results/Wk03_Section1_dev/reports/SDS-CP036-powercast_Wk03_Section1_Business_Report.md\n",
      "Consolidated report path: /home/6376f5a9-d12b-4255-9426-c0091ad440a7/Powercast/SDS-CP036-powercast_Wk03_Report_Business.md\n",
      "Done upserting Section 1.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Week 3 - Section 1: Model Selection & Training (Business)\n",
    "# Single Code Cell Execution - with Profiles: dev, preprod, final\n",
    "# Includes: Upsert into consolidated Week 3 report (no duplicates).\n",
    "\n",
    "import os, re, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def upsert_section_in_consolidated(consolidated_path: Path, section_text: str,\n",
    "                                   header_variants: list[str],\n",
    "                                   insert_order_hint: list[str] | None = None):\n",
    "    \"\"\"\n",
    "    Replace the block that starts with any header in `header_variants`.\n",
    "    If none exists, append (or insert before the first hint header, if provided).\n",
    "    Robust to '-' vs '–' and avoids inline regex flags collisions.\n",
    "    \"\"\"\n",
    "    consolidated_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    existing = consolidated_path.read_text(encoding=\"utf-8\") if consolidated_path.exists() else \"\"\n",
    "    text = existing.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "\n",
    "    # Build regex to remove any existing block for this section\n",
    "    hdr_alt = \"|\".join(re.escape(h) for h in header_variants)\n",
    "    any_sds_hdr = r\"^\\#\\s*SDS-CP036-powercast\\s*[–-]\\s*Week\\s*3\\s*Section\\s*\\d+:\\s*.*$\"\n",
    "    block_pat = rf\"^(?:{hdr_alt})\\s*.*?(?=^{any_sds_hdr}|\\Z)\"\n",
    "    text = re.sub(block_pat, \"\", text, flags=re.M | re.S).strip()\n",
    "\n",
    "    # Prepare the new (clean) block to insert\n",
    "    new_block = section_text.strip()\n",
    "\n",
    "    def insert_before_first_hint(container: str, block: str, hints: list[str]) -> str:\n",
    "        for h in hints:\n",
    "            m = re.search(rf\"^{re.escape(h)}\\s*$\", container, flags=re.M)\n",
    "            if m:\n",
    "                return container[:m.start()] + (block + \"\\n\\n---\\n\\n\") + container[m.start():]\n",
    "        return container + (\"\\n\\n---\\n\\n\" if container.strip() else \"\") + block\n",
    "\n",
    "    if insert_order_hint:\n",
    "        text = insert_before_first_hint(text, new_block, insert_order_hint)\n",
    "    else:\n",
    "        text = text + (\"\\n\\n---\\n\\n\" if text.strip() else \"\") + new_block\n",
    "\n",
    "    consolidated_path.write_text(text.strip() + \"\\n\", encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "# -------- Optional model deps (graceful fallback) --------\n",
    "PROPHET_AVAILABLE = True\n",
    "try:\n",
    "    from prophet import Prophet\n",
    "except Exception:\n",
    "    PROPHET_AVAILABLE = False\n",
    "\n",
    "XGB_AVAILABLE = True\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "except Exception:\n",
    "    XGB_AVAILABLE = False\n",
    "\n",
    "SARIMAX_AVAILABLE = True\n",
    "try:\n",
    "    from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "except Exception:\n",
    "    SARIMAX_AVAILABLE = False\n",
    "\n",
    "# -------- Project paths & helpers --------\n",
    "BASE_PROJECT_NAME = \"SDS-CP036-powercast\"\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    cur = start\n",
    "    for _ in range(10):\n",
    "        if (cur / \"data\").exists() and (cur / \"results\").exists():\n",
    "            return cur\n",
    "        cur = cur.parent\n",
    "    return start\n",
    "\n",
    "BASE_DIR = find_repo_root(Path.cwd())\n",
    "\n",
    "# -------- Profiles (dev, preprod, final) --------\n",
    "PROFILE = \"dev\"  # choose: \"dev\", \"preprod\", \"final\"\n",
    "\n",
    "profiles = {\n",
    "    \"dev\":     dict(FAST_MODE=True,  RESAMPLE_TO=\"H\", MAX_DAYS=365, TEST_DAYS=7,  BACKTEST=False, BACKTEST_FOLDS=0, BACKTEST_STEP_DAYS=0, BACKTEST_HOURS=0, PROPHET_ZONES=1),\n",
    "    \"preprod\": dict(FAST_MODE=False, RESAMPLE_TO=\"H\", MAX_DAYS=365, TEST_DAYS=28, BACKTEST=False, BACKTEST_FOLDS=0, BACKTEST_STEP_DAYS=0, BACKTEST_HOURS=0, PROPHET_ZONES=3),\n",
    "    \"final\":   dict(FAST_MODE=False, RESAMPLE_TO=\"H\", MAX_DAYS=365, TEST_DAYS=None, BACKTEST=True,  BACKTEST_FOLDS=6, BACKTEST_STEP_DAYS=7, BACKTEST_HOURS=168, PROPHET_ZONES=3),\n",
    "}\n",
    "cfg = profiles[PROFILE]\n",
    "\n",
    "FAST_MODE   = cfg[\"FAST_MODE\"]\n",
    "RESAMPLE_TO = cfg[\"RESAMPLE_TO\"]\n",
    "MAX_DAYS    = cfg[\"MAX_DAYS\"]\n",
    "TEST_DAYS   = cfg[\"TEST_DAYS\"]\n",
    "BACKTEST    = cfg[\"BACKTEST\"]\n",
    "BACKTEST_FOLDS = cfg[\"BACKTEST_FOLDS\"]\n",
    "BACKTEST_STEP_DAYS = cfg[\"BACKTEST_STEP_DAYS\"]\n",
    "BACKTEST_HOURS = cfg[\"BACKTEST_HOURS\"]\n",
    "PROPHET_ZONES = cfg[\"PROPHET_ZONES\"]\n",
    "\n",
    "# Results per profile (separate folders)\n",
    "RESULTS_DIR = BASE_DIR / \"results\" / f\"Wk03_Section1_{PROFILE}\"\n",
    "PLOTS_DIR   = RESULTS_DIR / \"plots\"\n",
    "REPORTS_DIR = RESULTS_DIR / \"reports\"\n",
    "for d in [RESULTS_DIR, PLOTS_DIR, REPORTS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------- Load & prepare data --------\n",
    "data_path = BASE_DIR / \"data\" / \"Tetuan City power consumption.csv\"\n",
    "if not data_path.exists():\n",
    "    raise FileNotFoundError(\"Dataset not found at: {}\".format(data_path))\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "# Normalize column names (strip & collapse spaces)\n",
    "df.columns = df.columns.str.strip().str.replace(r\"\\s+\", \" \", regex=True)\n",
    "\n",
    "# Parse time & set index\n",
    "df[\"DateTime\"] = pd.to_datetime(df[\"DateTime\"])\n",
    "df = df.set_index(\"DateTime\").sort_index()\n",
    "\n",
    "# Keep numeric cols only for modeling safety\n",
    "num_df = df.select_dtypes(include=[np.number]).copy()\n",
    "\n",
    "# Downsample & cap horizon\n",
    "if RESAMPLE_TO:\n",
    "    num_df = num_df.resample(RESAMPLE_TO).mean()\n",
    "if isinstance(MAX_DAYS, (int, float)):\n",
    "    try:\n",
    "        num_df = num_df.last(\"{}D\".format(int(MAX_DAYS)))\n",
    "    except Exception:\n",
    "        num_df = num_df.iloc[-24*int(MAX_DAYS):]\n",
    "\n",
    "# Zones (after header cleanup)\n",
    "zones = [\"Zone 1 Power Consumption\", \"Zone 2 Power Consumption\", \"Zone 3 Power Consumption\"]\n",
    "zones = [z for z in zones if z in num_df.columns]\n",
    "\n",
    "# -------- Helpers --------\n",
    "def mape_safe(y_true, y_pred):\n",
    "    denom = np.where(y_true == 0, np.nan, np.abs(y_true))\n",
    "    return float(np.nanmean(np.abs(y_true - y_pred) / denom) * 100.0)\n",
    "\n",
    "def evaluate_forecast(y_true, y_pred):\n",
    "    return {\n",
    "        \"RMSE\": float(mean_squared_error(y_true, y_pred, squared=False)),\n",
    "        \"MAE\": float(mean_absolute_error(y_true, y_pred)),\n",
    "        \"MAPE\": mape_safe(np.asarray(y_true), np.asarray(y_pred)),\n",
    "    }\n",
    "\n",
    "def plot_series(idx, y_true, y_pred, title, fname):\n",
    "    plt.figure(figsize=(11,4))\n",
    "    plt.plot(idx, y_true, label=\"Actual\")\n",
    "    plt.plot(idx, y_pred, label=\"Predicted\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.savefig(fname, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "def rolling_splits(index, folds=4, step_days=7, horizon_hours=168):\n",
    "    step = pd.Timedelta(days=step_days)\n",
    "    horizon = pd.Timedelta(hours=horizon_hours)\n",
    "    end = index.max()\n",
    "    splits = []\n",
    "    cur_test_end = end\n",
    "    for fold in range(folds):\n",
    "        test_start = cur_test_end - horizon + pd.Timedelta(hours=1)\n",
    "        val_end = test_start - pd.Timedelta(hours=1)\n",
    "        val_start = val_end - step + pd.Timedelta(hours=1)\n",
    "        train_end = val_start - pd.Timedelta(hours=1)\n",
    "        if train_end <= index.min():\n",
    "            break\n",
    "        tr_mask = (index <= train_end)\n",
    "        va_mask = (index >= val_start) & (index <= val_end)\n",
    "        te_mask = (index >= test_start) & (index <= cur_test_end)\n",
    "        splits.append((tr_mask, va_mask, te_mask))\n",
    "        cur_test_end = test_start - pd.Timedelta(hours=1)\n",
    "    return splits\n",
    "\n",
    "def prophet_predict(train_series, test_index):\n",
    "    df_p = train_series.reset_index()\n",
    "    df_p.columns = [\"ds\", \"y\"]\n",
    "    m = Prophet(daily_seasonality=True, weekly_seasonality=False, yearly_seasonality=False)\n",
    "    m.fit(df_p)\n",
    "    future = pd.DataFrame(test_index).reset_index()\n",
    "    future = future.rename(columns={future.columns[-1]: \"ds\"})\n",
    "    future[\"ds\"] = pd.to_datetime(future[\"ds\"])\n",
    "    fc = m.predict(future)\n",
    "    return fc[\"yhat\"].values\n",
    "\n",
    "def xgb_predict(train_series, val_series, test_series, lags=24, fast=FAST_MODE):\n",
    "    full_series = pd.concat([train_series, val_series, test_series])\n",
    "    df_l = pd.DataFrame({\"y\": full_series})\n",
    "    for L in range(1, lags+1):\n",
    "        df_l[\"lag_{}\".format(L)] = df_l[\"y\"].shift(L)\n",
    "    df_l = df_l.dropna()\n",
    "    n_train = len(train_series)\n",
    "    n_val = len(val_series)\n",
    "    split_idx = n_train + n_val - lags\n",
    "    train_ml = df_l.iloc[:split_idx]\n",
    "    test_ml  = df_l.iloc[split_idx:]\n",
    "    X_tr, y_tr = train_ml.drop(columns=[\"y\"]), train_ml[\"y\"]\n",
    "    X_te, y_te = test_ml.drop(columns=[\"y\"]), test_ml[\"y\"]\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=120 if fast else 200,\n",
    "        max_depth=4 if fast else 6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.9, colsample_bytree=0.9,\n",
    "        objective=\"reg:squarederror\", n_jobs=0\n",
    "    )\n",
    "    model.fit(X_tr, y_tr, verbose=False)\n",
    "    y_pred = model.predict(X_te)\n",
    "    y_true_idx = test_series.index[-len(y_pred):]\n",
    "    return y_true_idx, y_pred\n",
    "\n",
    "# -------- Experiment execution --------\n",
    "summary_rows = []\n",
    "\n",
    "try:\n",
    "    if not BACKTEST:\n",
    "        if TEST_DAYS is None:\n",
    "            TEST_DAYS = 7\n",
    "        test = num_df.last(\"{}D\".format(TEST_DAYS))\n",
    "        pre  = num_df.iloc[: -len(test)] if len(num_df) > len(test) else num_df.iloc[:0]\n",
    "        n_pre = len(pre)\n",
    "        n_train = int(n_pre * 0.85)\n",
    "        train = pre.iloc[:n_train]\n",
    "        val   = pre.iloc[n_train:]\n",
    "\n",
    "        preds_store = {}\n",
    "\n",
    "        for zone in zones:\n",
    "            # Baseline\n",
    "            if len(val) and len(test):\n",
    "                y_true = test[zone].values\n",
    "                pivot = val[zone].iloc[-1] if len(val) else train[zone].iloc[-1]\n",
    "                y_pred = np.repeat(pivot, len(test))\n",
    "                met = evaluate_forecast(y_true, y_pred)\n",
    "                summary_rows.append({\"Profile\": PROFILE, \"Zone\": zone, \"Model\": \"Baseline (Naive)\", **met})\n",
    "                preds_store[(\"Baseline\", zone)] = (test.index, y_pred)\n",
    "\n",
    "            # SARIMAX\n",
    "            if SARIMAX_AVAILABLE and len(train) > 10 and len(test) > 0:\n",
    "                try:\n",
    "                    seasonal = 24\n",
    "                    model = SARIMAX(train[zone], order=(1,1,1), seasonal_order=(0,1,1,seasonal),\n",
    "                                    enforce_stationarity=False, enforce_invertibility=False)\n",
    "                    res = model.fit(disp=False)\n",
    "                    fc = res.get_forecast(steps=len(test)).predicted_mean.values\n",
    "                    met = evaluate_forecast(test[zone].values, fc)\n",
    "                    summary_rows.append({\"Profile\": PROFILE, \"Zone\": zone, \"Model\": \"SARIMAX(1,1,1)(0,1,1,{})\".format(seasonal), **met})\n",
    "                    preds_store[(\"SARIMAX\", zone)] = (test.index, fc)\n",
    "                except Exception:\n",
    "                    summary_rows.append({\"Profile\": PROFILE, \"Zone\": zone, \"Model\": \"SARIMAX\", \"RMSE\": np.nan, \"MAE\": np.nan, \"MAPE\": np.nan})\n",
    "\n",
    "            # Prophet (optional)\n",
    "            if PROPHET_AVAILABLE and len(train) > 10 and len(test) > 0:\n",
    "                try:\n",
    "                    idx_zone = zones.index(zone)\n",
    "                    if (PROFILE == \"dev\" and idx_zone >= 1):\n",
    "                        pass\n",
    "                    else:\n",
    "                        yhat = prophet_predict(train[zone], test.index)\n",
    "                        met = evaluate_forecast(test[zone].values, yhat)\n",
    "                        summary_rows.append({\"Profile\": PROFILE, \"Zone\": zone, \"Model\": \"Prophet\", **met})\n",
    "                        preds_store[(\"Prophet\", zone)] = (test.index, yhat)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            # XGBoost (optional)\n",
    "            if XGB_AVAILABLE and len(train) > 10 and len(test) > 0:\n",
    "                try:\n",
    "                    idx_align, yhat = xgb_predict(train[zone], val[zone], test[zone], lags=24, fast=FAST_MODE)\n",
    "                    met = evaluate_forecast(test[zone].reindex(idx_align).values, yhat)\n",
    "                    summary_rows.append({\"Profile\": PROFILE, \"Zone\": zone, \"Model\": \"XGBoost (lags)\", **met})\n",
    "                    preds_store[(\"XGBoost\", zone)] = (idx_align, yhat)\n",
    "                except Exception:\n",
    "                    summary_rows.append({\"Profile\": PROFILE, \"Zone\": zone, \"Model\": \"XGBoost (lags)\", \"RMSE\": np.nan, \"MAE\": np.nan, \"MAPE\": np.nan})\n",
    "\n",
    "        results_df = pd.DataFrame(summary_rows)\n",
    "        results_csv = REPORTS_DIR / \"model_comparison.csv\"\n",
    "        results_df.to_csv(results_csv, index=False)\n",
    "\n",
    "        # Plot example for Zone 1\n",
    "        if \"Zone 1 Power Consumption\" in zones and len(results_df) > 0:\n",
    "            z = \"Zone 1 Power Consumption\"\n",
    "            z_df = results_df[results_df[\"Zone\"] == z].dropna(subset=[\"RMSE\"])\n",
    "            if len(z_df) > 0:\n",
    "                best_model = z_df.sort_values(\"RMSE\").iloc[0][\"Model\"]\n",
    "                key = None\n",
    "                for cand in [\"XGBoost\",\"SARIMAX\",\"Prophet\",\"Baseline\"]:\n",
    "                    if cand in best_model:\n",
    "                        key = (cand, z)\n",
    "                        break\n",
    "                if key in preds_store:\n",
    "                    idx, yhat = preds_store[key]\n",
    "                    ytrue = test[z].reindex(idx).values\n",
    "                    fname = PLOTS_DIR / \"Zone_1_best_model_forecast.png\"\n",
    "                    plot_series(idx, ytrue, yhat, \"Forecast Example - {} ({})\".format(z, best_model), fname)\n",
    "\n",
    "    else:\n",
    "        # Rolling backtest path\n",
    "        idx = num_df.index\n",
    "        splits = rolling_splits(idx, folds=BACKTEST_FOLDS, step_days=BACKTEST_STEP_DAYS, horizon_hours=BACKTEST_HOURS)\n",
    "        rows = []\n",
    "        for zone in zones:\n",
    "            for fold_id, (tr_m, va_m, te_m) in enumerate(splits, start=1):\n",
    "                train = num_df.loc[tr_m]\n",
    "                val   = num_df.loc[va_m]\n",
    "                test  = num_df.loc[te_m]\n",
    "\n",
    "                # Baseline\n",
    "                try:\n",
    "                    y_true = test[zone].values\n",
    "                    pivot = val[zone].iloc[-1] if len(val) else train[zone].iloc[-1]\n",
    "                    y_pred = np.repeat(pivot, len(test))\n",
    "                    met = evaluate_forecast(y_true, y_pred)\n",
    "                    rows.append({\"Profile\": PROFILE, \"Zone\": zone, \"Model\": \"Baseline (Naive)\", \"Fold\": fold_id, **met})\n",
    "                except Exception:\n",
    "                    rows.append({\"Profile\": PROFILE, \"Zone\": zone, \"Model\": \"Baseline (Naive)\", \"Fold\": fold_id, \"RMSE\": np.nan, \"MAE\": np.nan, \"MAPE\": np.nan})\n",
    "\n",
    "                # SARIMAX\n",
    "                if SARIMAX_AVAILABLE:\n",
    "                    try:\n",
    "                        if len(train) > 10:\n",
    "                            seasonal = 24\n",
    "                            model = SARIMAX(train[zone], order=(1,1,1), seasonal_order=(0,1,1,seasonal),\n",
    "                                            enforce_stationarity=False, enforce_invertibility=False)\n",
    "                            res = model.fit(disp=False)\n",
    "                            y_pred = res.get_forecast(steps=len(test)).predicted_mean.values\n",
    "                            met = evaluate_forecast(y_true, y_pred)\n",
    "                            rows.append({\"Profile\": PROFILE, \"Zone\": zone, \"Model\": \"SARIMAX(1,1,1)(0,1,1,{})\".format(seasonal), \"Fold\": fold_id, **met})\n",
    "                    except Exception:\n",
    "                        rows.append({\"Profile\": PROFILE, \"Zone\": zone, \"Model\": \"SARIMAX\", \"Fold\": fold_id, \"RMSE\": np.nan, \"MAE\": np.nan, \"MAPE\": np.nan})\n",
    "\n",
    "                # XGBoost\n",
    "                if XGB_AVAILABLE:\n",
    "                    try:\n",
    "                        def make_lag_df(series, lags=24):\n",
    "                            dfl = pd.DataFrame({\"y\": series})\n",
    "                            for L in range(1, lags+1):\n",
    "                                dfl[\"lag_{}\".format(L)] = dfl[\"y\"].shift(L)\n",
    "                            return dfl.dropna()\n",
    "                        LAGS = 24\n",
    "                        series = pd.concat([train[zone], val[zone], test[zone]])\n",
    "                        df_lag = make_lag_df(series, lags=LAGS)\n",
    "                        n_train_lag = len(train)\n",
    "                        n_val_lag = len(val)\n",
    "                        split_idx = n_train_lag + n_val_lag - LAGS\n",
    "                        train_ml = df_lag.iloc[:split_idx]\n",
    "                        test_ml  = df_lag.iloc[split_idx:]\n",
    "                        X_tr, y_tr = train_ml.drop(columns=[\"y\"]), train_ml[\"y\"]\n",
    "                        X_te, y_te = test_ml.drop(columns=[\"y\"]), test_ml[\"y\"]\n",
    "                        model = XGBRegressor(\n",
    "                            n_estimators=150, max_depth=5,\n",
    "                            learning_rate=0.08 if not FAST_MODE else 0.1,\n",
    "                            subsample=0.9, colsample_bytree=0.9,\n",
    "                            objective=\"reg:squarederror\", n_jobs=0\n",
    "                        )\n",
    "                        model.fit(X_tr, y_tr, verbose=False)\n",
    "                        y_pred = model.predict(X_te)\n",
    "                        y_true_idx = test.index[-len(y_pred):]\n",
    "                        y_true = test[zone].reindex(y_true_idx).values\n",
    "                        met = evaluate_forecast(y_true, y_pred)\n",
    "                        rows.append({\"Profile\": PROFILE, \"Zone\": zone, \"Model\": \"XGBoost (lags)\", \"Fold\": fold_id, **met})\n",
    "                    except Exception:\n",
    "                        rows.append({\"Profile\": PROFILE, \"Zone\": zone, \"Model\": \"XGBoost (lags)\", \"Fold\": fold_id, \"RMSE\": np.nan, \"MAE\": np.nan, \"MAPE\": np.nan})\n",
    "\n",
    "    # Save folds (if backtest)\n",
    "    if BACKTEST:\n",
    "        results_df = pd.DataFrame(rows)\n",
    "        results_csv = REPORTS_DIR / \"model_comparison_folds.csv\"\n",
    "        results_df.to_csv(results_csv, index=False)\n",
    "\n",
    "        agg = results_df.groupby([\"Zone\",\"Model\"]).agg(\n",
    "            RMSE_mean=(\"RMSE\",\"mean\"), RMSE_std=(\"RMSE\",\"std\"),\n",
    "            MAE_mean=(\"MAE\",\"mean\"),   MAE_std=(\"MAE\",\"std\"),\n",
    "            MAPE_mean=(\"MAPE\",\"mean\"), MAPE_std=(\"MAPE\",\"std\")\n",
    "        ).reset_index()\n",
    "        agg_csv = REPORTS_DIR / \"model_comparison_aggregate.csv\"\n",
    "        agg.to_csv(agg_csv, index=False)\n",
    "\n",
    "        champs = agg.sort_values([\"Zone\",\"RMSE_mean\"]).groupby(\"Zone\").head(1)\n",
    "        champs_csv = REPORTS_DIR / \"champion_models.csv\"\n",
    "        champs.to_csv(champs_csv, index=False)\n",
    "\n",
    "        # Optional champion plot\n",
    "        try:\n",
    "            if \"Zone 1 Power Consumption\" in zones and len(splits) > 0:\n",
    "                last_tr, last_va, last_te = splits[0]  # most recent fold\n",
    "                train = num_df.loc[last_tr]\n",
    "                val   = num_df.loc[last_va]\n",
    "                test  = num_df.loc[last_te]\n",
    "                z = \"Zone 1 Power Consumption\"\n",
    "                champ_model = champs[champs[\"Zone\"] == z].iloc[0][\"Model\"]\n",
    "                ytrue = test[z].values\n",
    "                idx = test.index\n",
    "                yhat = None\n",
    "                title = \"Forecast Example - {} ({})\".format(z, champ_model)\n",
    "                if \"SARIMAX\" in champ_model and SARIMAX_AVAILABLE:\n",
    "                    seasonal = 24\n",
    "                    model = SARIMAX(train[z], order=(1,1,1), seasonal_order=(0,1,1,seasonal),\n",
    "                                    enforce_stationarity=False, enforce_invertibility=False)\n",
    "                    res = model.fit(disp=False)\n",
    "                    yhat = res.get_forecast(steps=len(test)).predicted_mean.values\n",
    "                elif \"XGBoost\" in champ_model and XGB_AVAILABLE:\n",
    "                    y_true_idx, yhat = xgb_predict(train[z], val[z], test[z], lags=24, fast=FAST_MODE)\n",
    "                    ytrue = test[z].reindex(y_true_idx).values\n",
    "                    idx = y_true_idx\n",
    "                elif \"Baseline\" in champ_model:\n",
    "                    pivot = val[z].iloc[-1] if len(val) else train[z].iloc[-1]\n",
    "                    yhat = np.repeat(pivot, len(test))\n",
    "                if yhat is not None:\n",
    "                    fname = PLOTS_DIR / \"Zone_1_champion_forecast.png\"\n",
    "                    plot_series(idx, ytrue, yhat, title, fname)\n",
    "        except Exception:\n",
    "            pass\n",
    "finally:\n",
    "    # -------- Build Section Report (always) --------\n",
    "    section_report_path = REPORTS_DIR / \"SDS-CP036-powercast_Wk03_Section1_Business_Report.md\"\n",
    "    consolidated_report_path = BASE_DIR / \"SDS-CP036-powercast_Wk03_Report_Business.md\"\n",
    "\n",
    "    if not BACKTEST:\n",
    "        csv_line = \"[Model Comparison - CSV]({})\".format(\"model_comparison.csv\")\n",
    "        extra = []\n",
    "    else:\n",
    "        csv_line = \"[Model Comparison (folds) - CSV]({})\".format(\"model_comparison_folds.csv\")\n",
    "        extra = [\"[Aggregate Summary - CSV]({})\".format(\"model_comparison_aggregate.csv\"),\n",
    "                 \"[Champion Models - CSV]({})\".format(\"champion_models.csv\")]\n",
    "\n",
    "    lines = [\n",
    "        \"# {} - Week 3 Section 1: Model Selection & Training\".format(BASE_PROJECT_NAME),\n",
    "        \"\",\n",
    "        \"Profile: **{}**\".format(PROFILE),\n",
    "        \"\",\n",
    "        \"## Key Questions Answered\",\n",
    "        \"\",\n",
    "        \"Q: Which machine learning models did you choose for forecasting power consumption, and what motivated your selections?\",\n",
    "        \"A: We compared a simple Baseline (naive), a statistical time-series model (SARIMAX) that handles daily seasonality, an optional Prophet for trend/seasonality decomposition, and an optional XGBoost model with lag features for non-linear patterns.\",\n",
    "        \"\",\n",
    "        \"Q: How did you structure your models to handle the multi-zone prediction task (separate models vs. multi-output)?\",\n",
    "        \"A: We trained separate models for each zone (Zone 1, Zone 2, Zone 3). This avoids cross-zone interference and keeps insights clear for operations teams.\",\n",
    "        \"\",\n",
    "        \"Q: What challenges did you encounter during model training, and how did you address them?\",\n",
    "        \"A: Runtime and data quirks were the main issues. We used hourly resampling and capped history for speed; we normalized column headers to remove double spaces; and we set lighter SARIMAX and XGBoost defaults for fast, reliable runs. Prophet is optional and limited in dev to keep execution snappy.\",\n",
    "        \"\",\n",
    "        csv_line,\n",
    "    ] + ([\"\"] + extra if extra else []) + [\n",
    "        \"\",\n",
    "        \"---\",\n",
    "        \"\",\n",
    "        \"## Business Value Summary (Executive View)\",\n",
    "        \"- Faster iteration: Profiles (dev/preprod/final) let us move from quick smoke-tests to rigorous selection without changing code.\",\n",
    "        \"- Clear decisions: Side-by-side metrics identify the best model per zone; the final profile adds fold averages and stability checks.\",\n",
    "        \"- Reduced risk: Using RMSE/MAE/MAPE together prevents optimizing for a single number that might miss operational errors.\",\n",
    "        \"- Transparency: Reproducible splits, consistent resampling, and saved artifacts make results easy to explain to stakeholders.\",\n",
    "        \"- Scalability: The per-zone approach and shared pipeline scale cleanly as new data or zones are added.\",\n",
    "    ]\n",
    "\n",
    "    REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    with open(section_report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "    # ===== Upsert into consolidated (replace-or-append) =====\n",
    "    SECTION1_HEADERS = [\n",
    "        \"# SDS-CP036-powercast - Week 3 Section 1: Model Selection & Training\",\n",
    "        \"# SDS-CP036-powercast – Week 3 Section 1: Model Selection & Training\",\n",
    "    ]\n",
    "    SECTION2_HEADERS = [\n",
    "        \"# SDS-CP036-powercast - Week 3 Section 2: MLflow Experiment Tracking\",\n",
    "        \"# SDS-CP036-powercast – Week 3 Section 2: MLflow Experiment Tracking\",\n",
    "    ]\n",
    "    section_text = Path(section_report_path).read_text(encoding=\"utf-8\")\n",
    "    upsert_section_in_consolidated(\n",
    "        consolidated_path=consolidated_report_path,\n",
    "        section_text=section_text,\n",
    "        header_variants=SECTION1_HEADERS,\n",
    "        insert_order_hint=SECTION2_HEADERS  # keep S1 before S2\n",
    "    )\n",
    "\n",
    "    # Debug info\n",
    "    print(\"Section report path:\", section_report_path.resolve())\n",
    "    print(\"Consolidated report path:\", consolidated_report_path.resolve())\n",
    "    print(\"Done upserting Section 1.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbedc7b-1693-4ba3-9302-92440ebd90d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
