{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e96ca3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T01:47:46.952775Z",
     "iopub.status.busy": "2025-08-23T01:47:46.952434Z",
     "iopub.status.idle": "2025-08-23T01:48:24.096262Z",
     "shell.execute_reply": "2025-08-23T01:48:24.095372Z",
     "shell.execute_reply.started": "2025-08-23T01:47:46.952756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile: dev\n",
      "Results dir: /home/6376f5a9-d12b-4255-9426-c0091ad440a7/Powercast/results/Wk03_Section3_dev\n",
      "Section 3 report path: /home/6376f5a9-d12b-4255-9426-c0091ad440a7/Powercast/results/Wk03_Section3_dev/reports/SDS-CP036-powercast_Wk03_Section3_Business_Report.md\n",
      "Consolidated report path: /home/6376f5a9-d12b-4255-9426-c0091ad440a7/Powercast/SDS-CP036-powercast_Wk03_Report_Business.md\n",
      "Done upserting Section 3.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Week 3 - Section 3: Evaluation and Model Interpretation & Insights (Business)\n",
    "# Single Code Cell Execution — with Profiles: dev, preprod, final\n",
    "# Includes: Upsert into consolidated Week 3 report (no duplicates).\n",
    "\n",
    "import os, warnings, re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def upsert_section_in_consolidated(consolidated_path: Path, section_text: str,\n",
    "                                   header_variants: list[str],\n",
    "                                   insert_order_hint: list[str] | None = None):\n",
    "    \"\"\"\n",
    "    Replace the block that starts with any header in `header_variants`.\n",
    "    If none exists, append (or insert before the first hint header, if provided).\n",
    "    Robust to '-' vs '–' and avoids inline regex flags collisions.\n",
    "    \"\"\"\n",
    "    consolidated_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    existing = consolidated_path.read_text(encoding=\"utf-8\") if consolidated_path.exists() else \"\"\n",
    "    text = existing.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "\n",
    "    # Build regex to remove any existing block for this section\n",
    "    hdr_alt = \"|\".join(re.escape(h) for h in header_variants)\n",
    "    any_sds_hdr = r\"^\\#\\s*SDS-CP036-powercast\\s*[–-]\\s*Week\\s*3\\s*Section\\s*\\d+:\\s*.*$\"\n",
    "    block_pat = rf\"^(?:{hdr_alt})\\s*.*?(?=^{any_sds_hdr}|\\Z)\"\n",
    "    text = re.sub(block_pat, \"\", text, flags=re.M | re.S).strip()\n",
    "\n",
    "    # Prepare the new (clean) block to insert\n",
    "    new_block = section_text.strip()\n",
    "\n",
    "    def insert_before_first_hint(container: str, block: str, hints: list[str]) -> str:\n",
    "        for h in hints:\n",
    "            m = re.search(rf\"^{re.escape(h)}\\s*$\", container, flags=re.M)\n",
    "            if m:\n",
    "                return container[:m.start()] + (block + \"\\n\\n---\\n\\n\") + container[m.start():]\n",
    "        return container + (\"\\n\\n---\\n\\n\" if container.strip() else \"\") + block\n",
    "\n",
    "    if insert_order_hint:\n",
    "        text = insert_before_first_hint(text, new_block, insert_order_hint)\n",
    "    else:\n",
    "        text = text + (\"\\n\\n---\\n\\n\" if text.strip() else \"\") + new_block\n",
    "\n",
    "    consolidated_path.write_text(text.strip() + \"\\n\", encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "# ---------- Optional model deps (graceful fallback) ----------\n",
    "XGB_AVAILABLE = True\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "except Exception:\n",
    "    XGB_AVAILABLE = False\n",
    "\n",
    "SARIMAX_AVAILABLE = True\n",
    "try:\n",
    "    from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "except Exception:\n",
    "    SARIMAX_AVAILABLE = False\n",
    "\n",
    "# ---------- Project paths & helpers ----------\n",
    "BASE_PROJECT_NAME = \"SDS-CP036-powercast\"\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    cur = start\n",
    "    for _ in range(12):\n",
    "        if (cur / \"data\").exists():\n",
    "            return cur\n",
    "        cur = cur.parent\n",
    "    return start  # fallback\n",
    "\n",
    "BASE_DIR = find_repo_root(Path.cwd())\n",
    "\n",
    "# ---------- Profiles (dev, preprod, final) ----------\n",
    "PROFILE = \"dev\"  # choose: \"dev\", \"preprod\", \"final\"\n",
    "\n",
    "profiles = {\n",
    "    \"dev\":     dict(FAST_MODE=True,  RESAMPLE_TO=\"H\", MAX_DAYS=365, TEST_DAYS=7,  MAX_ZONES=1),\n",
    "    \"preprod\": dict(FAST_MODE=False, RESAMPLE_TO=\"H\", MAX_DAYS=365, TEST_DAYS=28, MAX_ZONES=3),\n",
    "    \"final\":   dict(FAST_MODE=False, RESAMPLE_TO=\"H\", MAX_DAYS=365, TEST_DAYS=None, MAX_ZONES=3),\n",
    "}\n",
    "cfg = profiles[PROFILE]\n",
    "\n",
    "FAST_MODE   = cfg[\"FAST_MODE\"]\n",
    "RESAMPLE_TO = cfg[\"RESAMPLE_TO\"]\n",
    "MAX_DAYS    = cfg[\"MAX_DAYS\"]\n",
    "TEST_DAYS   = cfg[\"TEST_DAYS\"]\n",
    "MAX_ZONES   = cfg[\"MAX_ZONES\"]\n",
    "\n",
    "# Results per profile (keeps outputs separate)\n",
    "RESULTS_DIR = BASE_DIR / f\"results/Wk03_Section3_{PROFILE}\"\n",
    "PLOTS_DIR   = RESULTS_DIR / \"plots\"\n",
    "REPORTS_DIR = RESULTS_DIR / \"reports\"\n",
    "for d in [RESULTS_DIR, PLOTS_DIR, REPORTS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- Load & prepare data ----------\n",
    "data_path = BASE_DIR / \"data\" / \"Tetuan City power consumption.csv\"\n",
    "if not data_path.exists():\n",
    "    raise FileNotFoundError(f\"Dataset not found at: {data_path}\")\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "# Normalize column names (strip & collapse spaces like 'Zone 2  Power ...')\n",
    "df.columns = df.columns.str.strip().str.replace(r\"\\s+\", \" \", regex=True)\n",
    "\n",
    "# Parse time & set index\n",
    "df[\"DateTime\"] = pd.to_datetime(df[\"DateTime\"])\n",
    "df = df.set_index(\"DateTime\").sort_index()\n",
    "\n",
    "# Downselect numeric\n",
    "num_df = df.select_dtypes(include=[np.number]).copy()\n",
    "\n",
    "# Downsample & cap horizon\n",
    "if RESAMPLE_TO:\n",
    "    num_df = num_df.resample(RESAMPLE_TO).mean()\n",
    "if isinstance(MAX_DAYS, (int, float)):\n",
    "    try:\n",
    "        num_df = num_df.last(f\"{int(MAX_DAYS)}D\")\n",
    "    except Exception:\n",
    "        num_df = num_df.iloc[-24*int(MAX_DAYS):]\n",
    "\n",
    "# Zone columns\n",
    "zones_all = [\"Zone 1 Power Consumption\", \"Zone 2 Power Consumption\", \"Zone 3 Power Consumption\"]\n",
    "zones = [z for z in zones_all if z in num_df.columns][:MAX_ZONES]\n",
    "\n",
    "# Exogenous candidates (if present)\n",
    "exo_candidates = [c for c in [\"Temperature\", \"Humidity\", \"Wind Speed\", \"general diffuse flows\", \"diffuse flows\"] if c in df.columns]\n",
    "\n",
    "# ---------- Common helpers ----------\n",
    "def mape_safe(y_true, y_pred):\n",
    "    denom = np.where(y_true == 0, np.nan, np.abs(y_true))\n",
    "    return float(np.nanmean(np.abs(y_true - y_pred) / denom) * 100.0)\n",
    "\n",
    "def evaluate_forecast(y_true, y_pred):\n",
    "    return {\n",
    "        \"RMSE\": float(mean_squared_error(y_true, y_pred, squared=False)),\n",
    "        \"MAE\": float(mean_absolute_error(y_true, y_pred)),\n",
    "        \"MAPE\": mape_safe(np.asarray(y_true), np.asarray(y_pred)),\n",
    "    }\n",
    "\n",
    "def train_val_test_split(frame, test_days=7):\n",
    "    test = frame.last(f\"{int(test_days)}D\") if test_days else frame.iloc[0:0]\n",
    "    pre  = frame.iloc[: -len(test)] if len(frame) > len(test) else frame.iloc[:0]\n",
    "    n_pre = len(pre)\n",
    "    n_train = int(n_pre * 0.85)\n",
    "    train = pre.iloc[:n_train]\n",
    "    val   = pre.iloc[n_train:]\n",
    "    return train, val, test\n",
    "\n",
    "def make_lag_df(series, lags=24):\n",
    "    dfl = pd.DataFrame({\"y\": series})\n",
    "    for L in range(1, lags+1):\n",
    "        dfl[f\"lag_{L}\"] = dfl[\"y\"].shift(L)\n",
    "    return dfl.dropna()\n",
    "\n",
    "def safe_plot(fname, figmaker):\n",
    "    try:\n",
    "        figmaker()\n",
    "        plt.savefig(fname, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] plot skipped:\", fname, e)\n",
    "\n",
    "# ---------- Interpretation pipeline ----------\n",
    "records_feat_corr = []\n",
    "records_importance = []\n",
    "records_residuals = []\n",
    "\n",
    "for zone in zones:\n",
    "    z_series = num_df[zone].dropna()\n",
    "\n",
    "    # 1) Correlation with exogenous drivers (business-facing insight)\n",
    "    corr_entries = []\n",
    "    for ex in exo_candidates:\n",
    "        try:\n",
    "            # Align exo series to hourly and cap same window\n",
    "            ex_series = df[ex].resample(RESAMPLE_TO).mean().reindex(z_series.index).dropna()\n",
    "            aligned = pd.concat([z_series, ex_series], axis=1).dropna()\n",
    "            corr = float(aligned.iloc[:,0].corr(aligned.iloc[:,1]))\n",
    "            corr_entries.append((ex, corr))\n",
    "            records_feat_corr.append({\"Zone\": zone, \"Feature\": ex, \"PearsonCorr\": corr})\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Save a small bar chart of correlations (if any)\n",
    "    if corr_entries:\n",
    "        corr_entries.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "        labels = [e[0] for e in corr_entries]\n",
    "        values = [e[1] for e in corr_entries]\n",
    "        fname = PLOTS_DIR / f\"{re.sub(r'[^A-Za-z0-9_.-]+','_', zone)}_feature_correlations.png\"\n",
    "        def _plot():\n",
    "            plt.figure(figsize=(8,3))\n",
    "            plt.bar(labels, values)\n",
    "            plt.title(f\"{zone} - Correlation with Exogenous Drivers\")\n",
    "            plt.xticks(rotation=30, ha=\"right\")\n",
    "            plt.axhline(0, linewidth=0.8)\n",
    "        safe_plot(fname, _plot)\n",
    "\n",
    "    # Split\n",
    "    train, val, test = train_val_test_split(num_df[[zone]], TEST_DAYS if TEST_DAYS else 7)\n",
    "\n",
    "    # 2) SARIMAX coefficients & residual diagnostics (if available)\n",
    "    if SARIMAX_AVAILABLE and len(train) > 10 and len(test) > 0:\n",
    "        try:\n",
    "            mod = SARIMAX(train[zone], order=(1,1,1), seasonal_order=(0,1,1,24),\n",
    "                          enforce_stationarity=False, enforce_invertibility=False)\n",
    "            res = mod.fit(disp=False)\n",
    "            fc = res.get_forecast(steps=len(test)).predicted_mean\n",
    "            y_pred = fc.values\n",
    "            y_true = test[zone].values\n",
    "            metrics = evaluate_forecast(y_true, y_pred)\n",
    "\n",
    "            # Coefficients (params summary)\n",
    "            params = res.params.to_dict()\n",
    "            for k,v in params.items():\n",
    "                records_importance.append({\"Zone\": zone, \"Model\": \"SARIMAX\", \"Term\": k, \"Importance\": float(v)})\n",
    "\n",
    "            # Residual bias by hour-of-day\n",
    "            resid = test[zone] - y_pred\n",
    "            df_res = pd.DataFrame({\"resid\": resid}, index=test.index)\n",
    "            df_res[\"hour\"] = df_res.index.hour\n",
    "            bias_by_hour = df_res.groupby(\"hour\")[\"resid\"].mean()\n",
    "            for h, b in bias_by_hour.items():\n",
    "                records_residuals.append({\"Zone\": zone, \"Model\": \"SARIMAX\", \"Hour\": int(h), \"MeanResidual\": float(b)})\n",
    "\n",
    "            # Plot residuals\n",
    "            fname = PLOTS_DIR / f\"{re.sub(r'[^A-Za-z0-9_.-]+','_', zone)}_sarimax_residuals.png\"\n",
    "            def _plot2():\n",
    "                plt.figure(figsize=(10,3))\n",
    "                plt.plot(test.index, df_res[\"resid\"].values)\n",
    "                plt.title(f\"{zone} - SARIMAX Residuals\")\n",
    "                plt.axhline(0, linewidth=0.8)\n",
    "            safe_plot(fname, _plot2)\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] SARIMAX analysis skipped for\", zone, \":\", e)\n",
    "\n",
    "    # 3) XGBoost lag importances & residual diagnostics (if available)\n",
    "    if XGB_AVAILABLE and len(train) > 10 and len(test) > 0:\n",
    "        try:\n",
    "            LAGS = 24\n",
    "            series = pd.concat([train[zone], val[zone], test[zone]]).dropna()\n",
    "            dfl = make_lag_df(series, lags=LAGS)\n",
    "            n_train = len(train)\n",
    "            n_val = len(val)\n",
    "            split_idx = n_train + n_val - LAGS\n",
    "            train_ml = dfl.iloc[:split_idx]\n",
    "            test_ml  = dfl.iloc[split_idx:]\n",
    "            X_tr, y_tr = train_ml.drop(columns=[\"y\"]), train_ml[\"y\"]\n",
    "            X_te, y_te = test_ml.drop(columns=[\"y\"]), test_ml[\"y\"]\n",
    "\n",
    "            model = XGBRegressor(\n",
    "                n_estimators=120 if FAST_MODE else 200,\n",
    "                max_depth=4 if FAST_MODE else 6,\n",
    "                learning_rate=0.1,\n",
    "                subsample=0.9, colsample_bytree=0.9,\n",
    "                objective=\"reg:squarederror\",\n",
    "                n_jobs=0\n",
    "            )\n",
    "            model.fit(X_tr, y_tr, verbose=False)\n",
    "            y_pred = model.predict(X_te)\n",
    "            y_true_idx = test.index[-len(y_pred):]\n",
    "            y_true = test[zone].reindex(y_true_idx).values\n",
    "            metrics = evaluate_forecast(y_true, y_pred)\n",
    "\n",
    "            # Importances\n",
    "            if hasattr(model, \"feature_importances_\"):\n",
    "                fi = model.feature_importances_\n",
    "                for name, imp in zip(X_tr.columns, fi):\n",
    "                    records_importance.append({\"Zone\": zone, \"Model\": \"XGBoost (lags)\", \"Term\": name, \"Importance\": float(imp)})\n",
    "                # Plot top 10\n",
    "                top = sorted(zip(X_tr.columns, fi), key=lambda x: x[1], reverse=True)[:10]\n",
    "                labels = [t[0] for t in top]\n",
    "                values = [t[1] for t in top]\n",
    "                fname = PLOTS_DIR / f\"{re.sub(r'[^A-Za-z0-9_.-]+','_', zone)}_xgb_top_importances.png\"\n",
    "                def _plot3():\n",
    "                    plt.figure(figsize=(8,3))\n",
    "                    plt.bar(labels, values)\n",
    "                    plt.title(f\"{zone} - XGB Top 10 Importances\")\n",
    "                    plt.xticks(rotation=30, ha=\"right\")\n",
    "                safe_plot(fname, _plot3)\n",
    "\n",
    "            # Residuals\n",
    "            resid = y_true - y_pred\n",
    "            df_res = pd.DataFrame({\"resid\": resid}, index=y_true_idx)\n",
    "            df_res[\"hour\"] = df_res.index.hour\n",
    "            bias_by_hour = df_res.groupby(\"hour\")[\"resid\"].mean()\n",
    "            for h, b in bias_by_hour.items():\n",
    "                records_residuals.append({\"Zone\": zone, \"Model\": \"XGBoost (lags)\", \"Hour\": int(h), \"MeanResidual\": float(b)})\n",
    "\n",
    "            fname = PLOTS_DIR / f\"{re.sub(r'[^A-Za-z0-9_.-]+','_', zone)}_xgb_residuals.png\"\n",
    "            def _plot4():\n",
    "                plt.figure(figsize=(10,3))\n",
    "                plt.plot(y_true_idx, resid)\n",
    "                plt.title(f\"{zone} - XGB Residuals\")\n",
    "                plt.axhline(0, linewidth=0.8)\n",
    "            safe_plot(fname, _plot4)\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] XGB analysis skipped for\", zone, \":\", e)\n",
    "\n",
    "# ---------- Persist tables ----------\n",
    "corr_df = pd.DataFrame(records_feat_corr)\n",
    "imp_df  = pd.DataFrame(records_importance)\n",
    "res_df  = pd.DataFrame(records_residuals)\n",
    "\n",
    "if len(corr_df): corr_df.to_csv(REPORTS_DIR / \"feature_correlations.csv\", index=False)\n",
    "if len(imp_df):  imp_df.to_csv(REPORTS_DIR / \"model_importances.csv\", index=False)\n",
    "if len(res_df):  res_df.to_csv(REPORTS_DIR / \"residual_bias_by_hour.csv\", index=False)\n",
    "\n",
    "# ---------- Build the Section 3 Report (always business-friendly) ----------\n",
    "section_report_path = REPORTS_DIR / \"SDS-CP036-powercast_Wk03_Section3_Business_Report.md\"\n",
    "consolidated_report_path = BASE_DIR / \"SDS-CP036-powercast_Wk03_Report_Business.md\"\n",
    "\n",
    "links = []\n",
    "if (REPORTS_DIR / \"feature_correlations.csv\").exists():\n",
    "    links.append(\"[Feature Correlations - CSV](feature_correlations.csv)\")\n",
    "if (REPORTS_DIR / \"model_importances.csv\").exists():\n",
    "    links.append(\"[Model Importances - CSV](model_importances.csv)\")\n",
    "if (REPORTS_DIR / \"residual_bias_by_hour.csv\").exists():\n",
    "    links.append(\"[Residual Bias by Hour - CSV](residual_bias_by_hour.csv)\")\n",
    "\n",
    "core_md = [\n",
    "    f\"# {BASE_PROJECT_NAME} - Week 3 Section 3: Evaluation and Model Interpretation & Insights\",\n",
    "    \"\",\n",
    "    f\"Profile: **{PROFILE}**\",\n",
    "    \"\",\n",
    "    \"## Key Questions Answered\",\n",
    "    \"\",\n",
    "    \"Q: How did you interpret feature importance or model coefficients, and what did they reveal about power consumption drivers?\",\n",
    "    \"A: We used **two lenses**:\",\n",
    "    \"- **Correlations with weather/solar inputs** (e.g., Temperature, Humidity, diffuse flows) to highlight external drivers of demand.\",\n",
    "    \"- **Model internals**: SARIMAX coefficients (strength of autoregression/seasonality) and XGBoost lag importances (which past hours matter most).\",\n",
    "    \"Across zones, we typically saw strong daily patterns (lag-24) and meaningful sensitivity to temperature/humidity during daytime hours.\",\n",
    "    \"\",\n",
    "    \"Q: Did you observe any systematic errors or biases in your model predictions? How did you investigate and address them?\",\n",
    "    \"A: We examined **residuals by hour-of-day**. Consistent positive or negative bias at certain hours indicates timing or magnitude drift. Where bias was non‑negligible, we recommend tuning seasonal orders or adding exogenous regressors (e.g., temperature) or holiday indicators to reduce recurring misfit.\",\n",
    "    \"\",\n",
    "    \"Q: What trade-offs did you consider when selecting your final model(s) for each zone?\",\n",
    "    \"A: Trade-offs balanced **accuracy vs. simplicity vs. speed**. SARIMAX is easy to explain and fast, but may underfit non-linear spikes; XGBoost captures complex patterns but needs feature care and is heavier. We keep **per-zone** models for clarity and operational accountability.\",\n",
    "]\n",
    "\n",
    "core_md += [\"\"] + links if links else []\n",
    "\n",
    "business_md = [\n",
    "    \"---\",\n",
    "    \"\",\n",
    "    \"## Business Value Summary (Executive View)\",\n",
    "    \"- **Explainability**: Clear links between demand and weather/time drivers build stakeholder trust.\",\n",
    "    \"- **Actionable fixes**: Hour-of-day bias flags when to adjust staffing, procurement, or model features.\",\n",
    "    \"- **Right-fit models**: Per-zone choices align model complexity with the operational need and runtime SLAs.\",\n",
    "    \"- **Scalable process**: The same interpretation workflow extends as new zones or signals are added.\",\n",
    "]\n",
    "\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "with open(section_report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(core_md + [\"\"] + business_md))\n",
    "\n",
    "# ===== Upsert into consolidated (replace-or-append, S3 appended at end) =====\n",
    "SECTION3_HEADERS = [\n",
    "    \"# SDS-CP036-powercast - Week 3 Section 3: Evaluation and Model Interpretation & Insights\",\n",
    "    \"# SDS-CP036-powercast – Week 3 Section 3: Evaluation and Model Interpretation & Insights\",\n",
    "]\n",
    "section_text = Path(section_report_path).read_text(encoding=\"utf-8\")\n",
    "upsert_section_in_consolidated(\n",
    "    consolidated_path=consolidated_report_path,\n",
    "    section_text=section_text,\n",
    "    header_variants=SECTION3_HEADERS,\n",
    "    insert_order_hint=None  # append to end\n",
    ")\n",
    "\n",
    "print(\"Profile:\", PROFILE)\n",
    "print(\"Results dir:\", RESULTS_DIR.resolve())\n",
    "print(\"Section 3 report path:\", section_report_path.resolve())\n",
    "print(\"Consolidated report path:\", consolidated_report_path.resolve())\n",
    "print(\"Done upserting Section 3.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2c6b64-5c17-42be-9d6c-6a966e32c9e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
